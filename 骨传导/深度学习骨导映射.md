
最开始，BILSTM 单说话人，需要归一化，除以max，然后求对数幅度谱的均值和方差，再归一化，求骨导和气导的，然后训练
最后需要用气导的均值和方差。

但是是单说话人的，多说话人效果不行。

试着用增强的思路，求mapping



WAVEUNET


DCCRN 求实部和虚部的MSE 但只能学习到4000HZ 4000HZ往上不行，试着改损失函数
加幅度谱的MSE 发现效果不行，高频不是有效信息，只是杂音。
选用多分辨率的STFTloss，结果比上一个好，但也是高频是杂音，且低频效果被影响

试着尝试搬运频谱的方式，类似带宽扩展，结果不行，和带宽扩展的论文所述不理想。集中处理搬移的频谱了，低频没管，高频也没什么表现

DPCRN loss为三个的MSE  但细节不太一样


如果数据不够  eben和seanet中有模拟数据的方法。频响曲线和互相关 TemporalTransforms 类中的一个方法，用于对音频信号进行低通滤波。它使用了一个双二阶滤波器，通过 filt-filt 技巧实现了零相移滤波。具体来说，它首先对音频信号进行了反转，然后使用 lowpass_biquad 函数进行滤波，最后再次反转得到最终结果。其中，滤波器的截止频率和品质因数可以通过参数 cutoff_freq 和 q_factor 进行设置。如果 determinist 参数为 False，则会在每次调用该方法时随机生成一个在 [0.8, 1.2] 范围内的系数，用于对截止频率和品质因数进行随机扰动。最后，该方法还对滤波后的信号进行了去噪处理，去除了滤波器引入的前向和反向延迟。

SEANET  谷歌的增强网络

尝试使用带宽扩展  
GAN网络 loss 时  detach 
EBEN 改进SEANET  里面audiounet结果结果和我之前crn类似，高频上不去，解释了值钱的原因  seanet也是做骨导气导融合增强的。
seanet生成器借鉴melgan 空洞卷积与跳跃连接 
# Melgan

残差层的空洞卷积，增加感受野，类似meigan hifigan 

[(41 封私信 / 7 条消息) MelGan - 搜索结果 - 知乎 (zhihu.com)](https://www.zhihu.com/search?type=content&q=MelGan)   讲解判别器结果，多尺度 连续下采样后输出feature 和 score  讲解eben以及melgan中用的铰链损失hinge loss（但github上的代码用的最小二乘损失）
替换featurematching_loss为multi-resolution stft loss会缩短收敛时间 

生成器loss  advloss 叫对抗loss，是score由判别器生成   featuremaploss，feature也是由判别器生成   辅助loss 就是正常的那种loss，比如multistft

# HIFIGAN

[细读经典：HiFiGAN，拥有多尺度和多周期判别器的高效声码器 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/406341310)   讲解网络，记忆weight norm eval时去掉它 
这篇文章讲的特别好 里面讲了网络为何设计，
里面讲到上采样容易出现棋盘效应  

## MRF 多感受野融合

为了避免棋盘效应因此每次转置卷积上采样之后，都会跟着一个多感受野融合multi-receptive field fusion (MRF)（MRF）的残差网络，以进一步提升样本点的生成质量  dilation=1,3,5 这就是seanet中的残差结构

# loss

自适应权重  函数的作用是计算动态调整重构损失和对抗损失的权重。具体来说，它计算了重构损失和对抗损失在某一层的梯度，并将它们的范数作为权重的分母和分子，然后将它们相除得到权重
![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230516/image.34g60oc5c540.webp)


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230522/image.3nm7b04viqi0.webp)


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230522/image.55ggt1pn24c0.webp)

里面讲解了**最小二乘损失**

[HiFiGAN (francis-komizu.github.io)](https://francis-komizu.github.io/notes/speech-synthesis/vocoder/hifigan/HiFiGAN.html)  里面讲判别器

多尺度和melgan一样，周期判别器是将音频隔着抽取，然后reshape成2d 进行2d的卷积

GAN网络训练困难，生成器loss上升，改成多次生成器，一次鉴别器试试 问了作者，GAN网络收敛不能根据loss判断，可根据客观指标。上升是正常的

BWE  HiFi-GAN+ 里面有作者GAN的训练技巧


或者不用GAN，直接用time或者freq的loss 效果不太好


骨气融合 SEANET   大象声科中的通入基频，fbank 方位角等特征学习幅度谱

# DPCRN

DPCRN  实时性比较好 尝试mask 和 mapping  
## InstanceNorm2d
DPCRN 模型中使用即时层归一化 (iLN)，InstanceNorm2d 而不是普通的 LN，其中所有帧在频率轴 f 和通道轴 c 上独立计算统计数据，并共享相同的可训练参数

具体差别看[深度学习中的Normalization方法 - 凌逆战 - 博客园 (cnblogs.com)](https://www.cnblogs.com/lxp-never/p/11566064.html#blogTitle5)

batchnorm 是在batch维度 layernorm是将channel w h 进行归一化  istancenorm是对w h 进行归一化

istancenorm 流式推理时 需要将track_running_stats 设置为true

音质修复 voicefixer  

一种是通过幅度谱，通过2d的unet卷积 得到mask  其中unet采用resunet 一种和seanet类似的二维卷积  可以看代码和论文

坐着提出一种两阶段方法 一阶段训练mel谱 用unet取卷积 得到好的mel谱 二阶段采用vocoder进行训练 

# BSRNN ：子带DPCRN 


![image](https://user-images.githubusercontent.com/123350717/214468836-54b8c5cf-a670-4bd9-add9-f95f48a4a673.png)

没有卷积 通过图可以看出来块间和块内 怎么操作
across T 块间 转换维度时T在中间

# 流式处理
istancenorm 需要将track_running_stats 设置为true
时间维度要在过去padding

[(33条消息) 李沐动手学深度学习V2-转置卷积和代码实现_cv_lhp的博客-CSDN博客](https://blog.csdn.net/flyingluohaipeng/article/details/125230277#:~:text=%E5%AF%B9%E8%BE%93%E5%85%A5%E7%9F%A9%E9%98%B5X%E5%92%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9F%A9%E9%98%B5K%E5%AE%9E%E7%8E%B0%E5%9F%BA%E6%9C%AC%E7%9A%84%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%EF%BC%8C%E4%B8%8E%E9%80%9A%E8%BF%87%E5%8D%B7%E7%A7%AF%E6%A0%B8%E2%80%9C%E5%87%8F%E5%B0%91%E2%80%9D%E8%BE%93%E5%85%A5%E5%85%83%E7%B4%A0%E7%9A%84%E5%B8%B8%E8%A7%84%E5%8D%B7%E7%A7%AF%E7%9B%B8%E6%AF%94%EF%BC%8C%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E9%80%9A%E8%BF%87%E5%8D%B7%E7%A7%AF%E6%A0%B8%E2%80%9C%E5%B9%BF%E6%92%AD%E2%80%9D%E8%BE%93%E5%85%A5%E5%85%83%E7%B4%A0%EF%BC%8C%E4%BB%8E%E8%80%8C%E4%BA%A7%E7%94%9F%E5%A4%A7%E4%BA%8E%E8%BE%93%E5%85%A5%E7%9A%84%E8%BE%93%E5%87%BA%E3%80%82%20import%20torch%20import%20d2l.torch,from%20torch%20import%20nn%20%23%E6%AD%A4%E5%AE%9E%E7%8E%B0%E6%98%AF%E5%9F%BA%E6%9C%AC%E7%9A%84%E4%BA%8C%E7%BB%B4%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E3%80%82)

转置卷积流式实现
```python
class StreamConvTranspose(nn.Module):
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 kernel_size: Union[int, Tuple[int, int]],
                 stride: Union[int, Tuple[int, int]] = 1,
                 padding: Union[str, int, Tuple[int, int]] = 0,
                 dilation: Union[int, Tuple[int, int]] = 1,
                 groups: int = 1,
                 bias: bool = True,
                 *args, **kargs):
        super(StreamConvTranspose, self).__init__(*args, **kargs)
        """
        流式转置卷积实现。
        默认 kernel_size = [T_size, F_size]
        默认 stride = [T_stride, F_stride] 且 T_stride == 1
        """
        self.in_channels = in_channels
        self.out_channels = out_channels
        if type(kernel_size) is int:
            self.T_size = kernel_size
            self.F_size = kernel_size
        elif type(kernel_size) in [list, tuple]:
            self.T_size, self.F_size = kernel_size
        else:
            raise ValueError('Invalid kernel size.')

        if type(stride) is int:
            self.T_stride = stride
            self.F_stride = stride
        elif type(stride) in [list, tuple]:
            self.T_stride, self.F_stride = stride
        else:
            raise ValueError('Invalid stride size.')

        assert self.T_stride == 1

        # 我们使用权重时间反向的Conv2d实现转置卷积
        self.Conv2d = nn.Conv2d(in_channels=in_channels,
                                out_channels=out_channels,
                                kernel_size=kernel_size,
                                stride=(self.T_stride, 1),  # F维度stride不为1，将在forward中使用额外的上采样算子
                                padding=padding,
                                dilation=dilation,
                                groups=groups,
                                bias=bias)

    @staticmethod
    def get_indices(inp, F_stride):
        """
        根据 input 的维度和 F维度上采样维度得到上采样之后的维度
        inp: [bs,C,T,F]
        return:
            indices: [bs,C,T,F]
        由于只对F上采样，因此输出的维度为 [bs,C,T,F_out]
        其中F_out = (F - 1) * (F_stride - 1) + F, 即向原来的每一个元素里面插入F_stride-1个零
        """
        bs, C, T, F = inp.shape
        # indices: [bs,C,T,F]
        F_out = (F - 1) * (F_stride - 1) + F
        indices = np.zeros([bs * 1 * T * F])
        index = 0
        for i in range(bs * 1 * T * F):
            indices[i] = index
            if (i + 1) % F == 0:
                index += 1
            else:
                index += F_stride
        indices = torch.from_numpy(np.repeat(indices.reshape([bs, 1, T, F]).astype('int64'), C, axis=1))
        return indices, F_out

    def forward(self, x, cache):
        """
        x: [bs,C,1,F]
        cache: [bs,C,T-1,F]
        """
        # [bs,C,T,F]
        inp = torch.cat([cache, x], dim=2)
        out_cache = inp[:, :, 1:]
        bs, C, T, F = inp.shape
        # 添加上采样算子
        if self.F_stride >= 1:
            # [bs,C,T,F] -> [bs,C,T,F,1] -> [bs,C,T,F,F_stride] -> [bs,C,T,F_out]
            inp = torch.concat([inp[:, :, :, :, None], torch.zeros([bs, C, T, F, self.F_stride - 1])], dim=-1).reshape(
                [bs, C, T, -1])
            left_pad = self.F_stride - 1
            if self.F_size > 1:
                if left_pad <= self.F_size - 1:
                    inp = torch.nn.functional.pad(inp, pad=[self.F_size - 1, self.F_size - 1 - left_pad, 0, 0])
                else:
                    inp = torch.nn.functional.pad(inp, pad=[self.F_size - 1, 0, 0, 0])[:, :, :,
                          : - (left_pad - self.F_stride + 1)]
            else:
                inp = inp[:, :, :, :-left_pad]

        outp = self.Conv2d(inp)
        # 这里也可以输出x，把更新cache放到外面

        return outp, out_cache
```


权重
```python
torch.flip(Conv_dict["real_decoder.real_dconv_1.weight"].permute([1, 0, 2, 3]), dims=[-2, -1])
```