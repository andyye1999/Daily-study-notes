卷积层，池化层计算公式：(输入图片大小-卷积核大小+2*padding)/strides+1 


![tupuan](https://pic1.zhimg.com/80/v2-5e5f687e78edd13e572039f5132f4248_720w.webp)

# RNN
[RNN LSTM GRU 与梯度爆炸梯度消失](https://zhuanlan.zhihu.com/p/28297161)



# LSTM


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230410/image.7fj9tyuzl0o0.webp)


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230405/image.4meykpdfxso0.webp)


![](https://pic2.zhimg.com/80/v2-ec148fed5f9397087e0aae2b7e05d241_720w.webp)

  
![](https://pic4.zhimg.com/80/v2-c50de47c672cfec7cd6671d3dc6a2147_720w.webp)


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230410/image.6vbfdnnthi00.webp)

### LSTM 是如何实现长短期记忆的？（遗忘门和输入门的作用）

-   LSTM 主要通过**遗忘门**和**输入门**来实现长短期记忆。
    -   如果当前时间点的状态中没有重要信息，遗忘门 `f` 中各分量的值将接近 1（`f -> 1`）；输入门 `i` 中各分量的值将接近 0（`i -> 0`）；此时过去的记忆将会被保存，从而实现**长期记忆**；
    -   如果当前时间点的状态中出现了重要信息，且之前的记忆不再重要，则 `f -> 0`，`i -> 1`；此时过去的记忆被遗忘，新的重要信息被保存，从而实现**短期记忆**；
    -   如果当前时间点的状态中出现了重要信息，但旧的记忆也很重要，则 `f -> 1`，`i -> 1`。


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230811/image.9hs9n0n810g.webp)

![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230811/image.cq63w1zw6iw.webp)
# gru
torch.nn.GRU
输入：
(input_dim ,hidden_dim ,num_layers ，…)
– input_dim 表示输入的特征维度
– hidden_dim 表示输出的特征维度，如果没有特殊变化，相当于out
– num_layers 表示网络的层数
– nonlinearity 表示选用的非线性**函数，默认是 ‘tanh’
– bias 表示是否使用偏置，默认使用
– batch_first 表示输入数据的形式，默认是 False，[即(序列长度seq,批大小batch,特征维度feature)];若True则(batch,seq,feature)
– dropout 缺省值为0,表示不使用dropout层;若为1，则除最后一层外，其它层的输出都会加dropout层
– bidirectional 表示是否使用双向的 rnn，默认是 False
输出：out和 ht
out的输出维度：[seq_len,batch_size,output_dim]
ht的维度：[num_layers * num_directions, batch_size, hidden_size],num_directions=1，单向，取值2时为双向，num_layers为层数
out[-1]=ht[-1]


在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控 zz （update gate）。

**更新表达式**： ht=(1−z)⊙ht−1+z⊙h′h^t = (1-z) \odot h^{t-1} + z\odot h'

首先再次强调一下，门控信号（这里的 zz ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。

**我们使用了同一个门控 � 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）**。  



![A gated recurrent unit neural network.](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230410/image.6y51ii7mzyg0.webp)


## GRU 与 LSTM 的关系

GRU 认为 LSTM 中的**遗忘门**和**输入门**的功能有一定的重合，于是将其合并为一个**更新门**

```c
void compute_gru(const GRULayer *gru, float *state, const float *input)
{
   int i;
   int N, M;
   int stride;
   float tmp[MAX_NEURONS];
   float z[MAX_NEURONS];
   float r[MAX_NEURONS];
   float h[MAX_NEURONS];
   celt_assert(gru->nb_neurons <= MAX_NEURONS);
   celt_assert(input != state);
   M = gru->nb_inputs;
   N = gru->nb_neurons;
   stride = 3*N;
   /* Compute update gate. */
   for (i=0;i<N;i++)
      z[i] = gru->bias[i];
   if (gru->reset_after)
   {
      for (i=0;i<N;i++)
         z[i] += gru->bias[3*N + i];
   }
   sgemv_accum(z, gru->input_weights, N, M, stride, input);
   sgemv_accum(z, gru->recurrent_weights, N, N, stride, state);
   compute_activation(z, z, N, ACTIVATION_SIGMOID);

   /* Compute reset gate. */
   for (i=0;i<N;i++)
      r[i] = gru->bias[N + i];
   if (gru->reset_after)
   {
      for (i=0;i<N;i++)
         r[i] += gru->bias[4*N + i];
   }
   sgemv_accum(r, &gru->input_weights[N], N, M, stride, input);
   sgemv_accum(r, &gru->recurrent_weights[N], N, N, stride, state);
   compute_activation(r, r, N, ACTIVATION_SIGMOID);

   /* Compute output. */
   for (i=0;i<N;i++)
      h[i] = gru->bias[2*N + i];
   if (gru->reset_after)
   {
      for (i=0;i<N;i++)
         tmp[i] = gru->bias[5*N + i];
      sgemv_accum(tmp, &gru->recurrent_weights[2*N], N, N, stride, state);
      for (i=0;i<N;i++)
         h[i] += tmp[i] * r[i];
      sgemv_accum(h, &gru->input_weights[2*N], N, M, stride, input);
   } else {
      for (i=0;i<N;i++)
         tmp[i] = state[i] * r[i];
      sgemv_accum(h, &gru->input_weights[2*N], N, M, stride, input);
      sgemv_accum(h, &gru->recurrent_weights[2*N], N, N, stride, tmp);
   }
   compute_activation(h, h, N, gru->activation);
   for (i=0;i<N;i++)
      h[i] = z[i]*state[i] + (1-z[i])*h[i];
   for (i=0;i<N;i++)
      state[i] = h[i];
}
void compute_dense(const DenseLayer *layer, float *output, const float *input)
{
   int i;
   int N, M;
   int stride;
   M = layer->nb_inputs;
   N = layer->nb_neurons;
   stride = N;
   celt_assert(input != output);
   for (i=0;i<N;i++)
      output[i] = layer->bias[i];
   sgemv_accum(output, layer->input_weights, N, M, stride, input);
   compute_activation(output, output, N, layer->activation);
}
这段代码实现了一个全连接层的前向传播。输入参数包括一个指向DenseLayer结构体的指针，一个指向输出的指针和一个指向输入的指针。其中DenseLayer结构体包含了该层的权重、偏置、激活函数等信息。该函数首先将输出初始化为偏置，然后使用sgemvaccum函数计算输入和权重的乘积并累加到输出中。最后，使用computeactivation函数对输出进行激活函数处理。其中sgemvaccum函数实现了矩阵向量乘法的累加版本，computeactivation函数实现了不同激活函数的处理。
static void sgemv_accum(float *out, const float *weights, int rows, int cols, int col_stride, const float *x)
{
   int i, j;
   if (rows % 16 == 0)
   {
      sgemv_accum16(out, weights, rows, cols, col_stride, x);
   } else {
      for (i=0;i<rows;i++)
      {
         for (j=0;j<cols;j++)
            out[i] += weights[j*col_stride + i]*x[j];
      }
   }
}
这段代码实现了一个矩阵向量乘法的累加操作。其中，out是输出向量，weights是输入矩阵，rows是矩阵的行数，cols是矩阵的列数，col_stride是矩阵列之间的跨度，x是输入向量。如果rows是16的倍数，则调用sgemv_accum16函数进行计算，否则使用两层循环计算。在循环中，out[i]表示输出向量的第i个元素，weights[j*col_stride + i]表示输入矩阵第j列第i行的元素，x[j]表示输入向量的第j个元素。
```

```python
import torch
import torch.nn as nn


# 例子1，单向一层网络
embed = nn.Embedding(3, 50) #一共3个词，每个词的词向量维度设置为50维
x = torch.LongTensor([[0, 1, 2]]) # 3个句子，每个句子只有一个词，对应的索引分别时0，1，2
x_embed = embed(x)
print(x_embed.size())
# torch.Size([1, 3, 50]) # [规整后的句子长度，样本个数（batch_size）,词向量维度]

gru = nn.GRU(input_size=50, hidden_size=50) # 词向量维度，隐藏层维度
out, hidden = gru(x_embed)

print(out.size())
# torch.Size([1, 3, 50]) # [seq_len,batch_size,output_dim]

print(hidden.size())
# torch.Size([1, 1, 50]) # [num_layers * num_directions, batch_size, hidden_size]


# 例子2，单向2层网络
gru_seq = nn.GRU(10, 20,2) # x_dim,h_dim,layer_num
gru_input = torch.randn(3, 32, 10) # seq_len,batch_size,x_dim
out, h = gru_seq(gru_input)
print(out.size())
print(h.size())

'''
torch.Size([3, 32, 20]) # [seq_len,batch_size,output_dim]
torch.Size([2, 32, 20]) # [num_layers * num_directions, batch_size, hidden_size]

'''

```

# conv1d
![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20221120/image.18x6t933e29s.webp)


dilation 膨胀卷积，跨步