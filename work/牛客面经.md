

# 直线麦克风阵列怎么分左右

面阵，十字阵才能分

# 频率混叠 栅栏效应

如果不满足采样定理，则会发生频率折叠现象。

采样定理告诉我们，对于一个最高频率为 $f_{\text{max}}$ 的连续信号，其最高可以采样频率为 $2f_{\text{max}}$。如果采样频率小于 $2f_{\text{max}}$，则会发生频率折叠。

频率折叠的现象是：在频域中的高频部分会折叠到低频部分，造成频率冗余和信号失真。因此，我们在采样信号时必须遵循采样定理，以确保采样后的信号能够准确反映原始信号的频率特征。以上内容由chatgpt回答

栅栏效应是因为DFT计算的频谱被限制在基频的整数倍而不可能将频谱视为一个连续函数而产生的。就一定意义而言，栅栏效应表现为用DFT计算整个频谱时，就好像通过一个“栅栏”来观看一个图景一样，只能在离散点的地方看到真实图景。

增加频域抽样点数N，同时在不改变时域数据的情况下，在时域数据末端添加一些零值点，使得谱线更密，这样就可以减小栅栏效应，观察到原来看不到的频谱分量。注意，该方法通过补零来增加N，此时采样频率f(s)会随之成正比上升，又由于频率分辨率F=f(s)/N，频率分辨率不改变，也就是说，补零不改变频率分辨率。

看这个
**[[语音信号基础知识#频谱泄露和混叠]]**

# 抽取与插值

[[信号的抽取与插值与子带滤波器组]]


# RNN GRU LSTM

[[深度学习#RNN]]

# RNN如何解决梯度问题

[Algorithm_Interview_Notes-Chinese/B-专题-RNN.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/B-%E4%B8%93%E9%A2%98-RNN.md#rnn-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8)


# FIR IIR


滤波器可分为两种，IIR（无限冲激响应）滤波器和FIR（有限冲激响应）滤波器。

**（1）FIR和IIR滤波器的不同**

FIR滤波器的冲激响应在有限时间内衰减为0，输出仅取决于当前和过去的输入信号值，在Z域上其极点位置只能是原点，而IIR滤波器的冲激响应会无限持续，输出不仅取决于当前和过去的输入信号，还和过去的输出有关，IIR的极点可以处于单位圆内任何地方。  
设计同样参数的滤波器，FIR要比IIR需要更多的参数，也就是在处理时需要更长的时间去计算，实时性差一些。  
FIR具有线性相位，IIR不具有，非线性相位是指对于不同的频率分量造成的相位差与频率不成比例，使得输出时不同频率分量的叠加的相位值和输入时有变化，从而导致了信号的失真。因此在进行IIR设计的时候需考虑这些，如有相位要求需添加相位校准网络。  
在实际应用中，如果滤波器通带内不要求线性相位，则使用IIR，若有要求，则根据相位失真度、计算量、复杂度等因素综合考虑是选择FIR还是选择IIR+相位补偿。

**（2）FIR和IIR设计方法**

1FIR：窗函数法、频率采样法、切比雪夫逼近法。对比这三种方法，窗函数法是最早提出的，缺少关键频率的精度控制，如用该种方法设计的低通滤波器，它的截止频率依赖于窗函数的类型和滤波器长度M，并不能从截止频率出发进行设计。频率采样法指定了一些w处H（w）的值，并规定了过渡带为2π/M的倍数，由于这种H（w）在过渡带以外的其他频率上为0或1的曲线特性，这种方法用于实现频域滤波。切比雪夫逼近法在技术指标的管控上比前两种都要好，可以按照参数wp，ws，δ1，δ2，M给定的技术指标，指定参数wp，ws，δ1，M，然后利用δ2来优化滤波器，这种方法后续可着重研究一下。  
IIR：由于模拟滤波器是一个充分研究的成熟领域，多使用模拟滤波器转换到数字滤波器上。导数逼近法、冲击不变法、双线性变换法。前两种方法有严重的局限性，仅适合于低通滤波器和一类有限的带通滤波器，双线性变换法则无此限制。常用的模拟滤波器有巴特沃斯滤波器、切比雪夫滤波器、椭圆滤波器、贝塞尔滤波器。


# 重采样

[[信号的抽取与插值与子带滤波器组]]
[[语音基础/重采样]]

# MFCC

[[MFCC]]


![](https://pic1.zhimg.com/80/v2-2bd74388bcd020fbaeff2a99e24ef800_720w.webp)


MFCC和FBANK MFCC在FBANK做DCT ，DCT是线性变换，容易损失FBANK非线性信息。DCT相当于去相关 FBANK相关性高（相邻滤波器组有重叠）MFCC具有更好的判别度，适合做特征。MFCC计算量大

为什么取13阶MFCC
选择前13个MFCC系数的原因是：

-   [经验表明，少于13个系数会影响识别性能，多于13个系数对性能的提高不明显](https://www.zhihu.com/question/24502675)[1](https://www.zhihu.com/question/24502675)。
-   [13个系数是一种习惯，也可以选择12或14](https://www.zhihu.com/question/24502675)[1](https://www.zhihu.com/question/24502675)[2](https://blog.csdn.net/weixin_42788078/article/details/101422144)。
-   [DCT的作用是获得频谱的倒谱，倒谱的低频分量就是频谱的包络，倒谱的高频分量就是频谱的细节，这些都是语音识别上有效的语音物理信息](https://blog.csdn.net/weixin_42788078/article/details/101422144)[2](https://blog.csdn.net/weixin_42788078/article/details/101422144)[3](https://www.zhihu.com/question/24502690)。
-   [为了平衡识别准确度和计算速度，以及去除不需要的信息](https://www.zhihu.com/question/24502690)[3](https://www.zhihu.com/question/24502690)。

# 频率分辨率

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.rns6p7jmqxs.webp)

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.1etnrunmea8.webp)

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.3j2cww9uekm0.webp)


![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.359syy3mfym0.webp)

频率分辨率与反比于**模拟信号的长度T**

时间分辨率与频率分辨率相互制约，可以用矩形窗和sinc函数解释，矩形窗越大，sinc越接近脉冲
计算分辨率计算时fs是不变的，但补零会导致fs变大，此时采样频率f(s)会随之成正比上升，又由于频率分辨率F=f(s)/N，频率分辨率不改变，也就是说，补零不改变频率分辨率。

# 相关与卷积的区别与联系

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.701o64broe00.webp)



# 圆周卷积和线性卷积

[[线性卷积圆周卷积]]

# 预加重

[[预加重]]  预加重 一阶高通滤波器 **语音能量集中在低频部分，造成语音信号高频端信噪比可能低 预加重增加语音高频分量，之后去加重减小噪声高频分量**
1-0.8z^-1 

# 加窗 不同窗特点


[[语音信号基础知识#加窗]]

主瓣宽度越窄越好，旁瓣衰减越大越好

对连续的语音分帧做STFT处理，等价于截断一段时间信号，对其进行周期性延拓，从而变成无限长序列，并对该无限长序列做FFT变换，这一截断并不符合傅里叶变换的定义。因此，会导致频谱泄露和混叠

频谱泄露会导致幅度较小的频点淹没在幅度较大的频点泄露分量中，

而混叠会在分段拼接处引入虚假的峰值，进而不能获得准确的频谱情况

加窗是为了抑制频谱泄露和混叠的产生

不同的窗函数有不同的特点。一般来说，选择窗函数要考虑的因素有：

1.  带噪比：窗函数的形状直接影响语音分帧的带噪比，比如矩形窗带来的噪音大于汉宁窗。
    
2.  稳定性：窗函数的形状直接影响语音分帧的稳定性，比如汉宁窗与语音信号更加稳定。
    
3.  计算复杂度：不同的窗函数具有不同的计算复杂度，例如汉宁窗与矩形窗。
    

常见的窗函数包括：矩形窗、汉宁窗、Hann窗、Hamming窗、Blackman窗等。每种窗函数的适用场景都不同，取决于应用需求。以上不同窗特点为chatGPT回答

# 自适应滤波

[[FDAF自适应滤波器算法综述]]

# 推导FFT

看书

# 实数FFT

[[fft与实数fft]]

# FFT复杂度

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230108/image.35gw6tlg3980.webp)

分子DFT复杂度 N方复数乘法 N(N-1)复数加法 一次复数乘法等于四次实数乘两次实数加
# 损失函数的设计思路，以及常见的损失函数

  
[模型训练——Loss函数](https://zhuanlan.zhihu.com/p/436809988)

[[深度学习中的激活函数]]


[Algorithm_Interview_Notes-Chinese/A-深度学习基础.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)


 **`ReLU` 相比 `sigmoid` 的优势 (3)**

1.  **避免梯度消失*****
    -   `sigmoid`函数在输入取绝对值非常大的正值或负值时会出现**饱和**现象——在图像上表现为变得很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失；
    -   `ReLU` 的导数始终是一个常数——负半区为 0，正半区为 1——所以不会发生梯度消失现象
2.  **减缓过拟合****
    -   `ReLU` 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——**稀疏激活**
    -   这有助于减少参数的相互依赖，缓解过拟合问题的发生
3.  **加速计算***
    -   `ReLU` 的求导不涉及浮点运算，所以速度更快


**sidir**    [[conv-tasnet#si-snr]]


多分辨率stftloss 各种time和freq的组合

# 梯度爆炸和梯度消失问题的成因和缓解

梯度爆炸的成因： 在深度学习中，梯度爆炸是指在计算损失函数的梯度时，因某些原因使得梯度值变得非常大，从而导致训练无法正常进行。通常发生在链式求导时，因为每层的梯度都是从上一层传递下来的，当上一层的梯度值很大时，就可能导致梯度爆炸。

梯度消失的成因： 梯度消失是指在计算损失函数的梯度时，因某些原因使得梯度值变得非常小，从而导致训练无法正常进行。通常发生在使用非线性激活函数（如sigmoid）的网络中，当激活函数的输入较大时，激活函数的导数很小，从而导致梯度值很小。

缓解梯度爆炸和梯度消失的方法：

-   使用更多的Batch Normalization
-   使用更大的学习率
-   使用更小的模型
-   在链式求导的过程中使用更强的正则化方法，如Dropout
-   使用更复杂的激活函数，如ReLU
-   使用更好的优化器，如Adam


（1）梯度裁剪  
梯度裁剪主要是针对梯度爆炸提出。其思想也比较简单，训练时候设置一个阈值，梯度更新的时候，如果梯度超过阈值，那么就将梯度强制限制在该范围内，这时可以防止梯度爆炸。  
权重正则化（weithts regularization)也可以解决梯度爆炸的问题，其思想就是我们常见的正则方式。

![](https://pic4.zhimg.com/80/v2-d7636acbe4530c258747b76cd49fa74f_720w.webp)

α是正则化系数。如果发生梯度爆炸，||w||的平方会变得非常大，这样就可以一定程度避免梯度爆炸。  
（2）relu等激活函数  
relu我们就非常常见了，在AlexNet网络中最先提出。relu激活函数的导数为1，那么就不存在梯度消失爆炸的问题，不同层之间的梯度基本保持一致。  
而relu的缺点则是，负数部分恒为0，所以存在一定‘死区’，会导致一些神经元无法被激活，可以通过elu等来改善死区的问题。  
（3）batch normalization  
batch normalization目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果.BN本质上是解决反向传播过程中的梯度问题，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。  
在我们前面推导的反向传播求导公式中，含有w项，所以w的大小影响了梯度的消失和爆炸。BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。  
（4）ResNet 残差结构  
Residual Net中包含残差的shortcut(捷径)部分，shortcut的网络结构为

![](https://pic2.zhimg.com/80/v2-0d05ff3e2afe662a9d38f011ecaa01f5_720w.webp)

残差网络主要是为了解决梯度消失的问题。从上面的网络结构可以看出，由于shortcut的存在，残差网络的输出在对于输入求导时，总有一个x保证有一个常数梯度1(除非F(X)刚好求导为-1这样导数求和为0，但这种概率太小)，所以一定程度能解决梯度消失的问题。如果从输入到输出，恒等映射是最优解，那么将残差F(x)直接设置为0即可。
以上由chatgpt回答

[算法岗常见面试题（五）：梯度消失和梯度爆炸_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/470250150548545536?sourceSSR=users)

防止梯度爆炸：

1.  梯度剪切：更新梯度时，梯度超过某个阈值，就将其强制限制在这个范围内
2.  权重正则化：L1正则和L2正则

防止梯度消失：

1.  合理的激活函数（如ReLU）+权重初始化
2.  Batch Normalization：应用于每层激活函数之前
3.  残差网络

# 几种不同的归一化方式的区别和联系

归一化是一种常见的数据预处理方法，目的是将数据转换为固定范围内的数值。常用的几种归一化方式有以下几种：

1.  Min-Max 归一化：这种归一化方法通过将数据的最大值和最小值缩放到0-1的范围内。公式为：x' = (x-x_min)/(x_max-x_min)
    
2.  Z-Score 归一化：这种归一化方法通过计算数据的均值和标准差，将数据标准化为均值为0，标准差为1的正态分布数据。公式为：x' = (x-μ)/σ
    
3.  小数定标归一化：这种归一化方法通过计算数据的最大的整数位数，将数据的最大值归一化为1，其他数据相对地缩放。
    
4.  极差归一化：这种归一化方法通过计算数据的极差，将数据缩放到0-1的范围内。公式为：x' = (x-x_min)/(x_max-x_min)


以上四种归一化方式各有优劣，具体选择哪种归一化方式取决于数据的特征、数据处理目的以及其他因素。此外，不同归一化方式在模型训练时可能会影响模型的训练结果，因此需要根据具体场景进行选择。

以上由chatgpt回答

# 音量的归一化


避免非因果 递归归一化
![image](https://cdn.staticaly.com/gh/andyye1999/picx-images-hosting@master/20230405/image.35vrfxh4i5u0.webp)


或者在损失函数归一化
![image](https://cdn.staticaly.com/gh/andyye1999/picx-images-hosting@master/20230405/image.2xq54ule4cs0.webp)
# dropout的原理，训练和推理的不同

Dropout是一种防止神经网络过拟合的技术，它的原理是在训练过程中随机丢弃一部分神经元，从而减少神经元之间的共适应关系，增强网络的泛化能力。

训练和推理的不同在于，训练时每个神经元以一定的概率p被置为0，而推理时所有的神经元都是激活的，但是要对输出进行缩放，乘以1-p或者p，以保持输出的期望不变。

训练时按照概率P丢弃（特征置0）其实就是让它的激活函数值以概率p变为0. 测试时所有特征xP

[深度学习中Dropout原理解析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/38200980)

# batchnorm

[Algorithm_Interview_Notes-Chinese/A-深度学习基础.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md#batch-normalization%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96)


# 过拟合与欠拟合

-   **欠拟合**指模型不能在**训练集**上获得足够低的**训练误差**；
-   **过拟合**指模型的**训练误差**与**测试误差**（泛化误差）之间差距过大；反映在**评价指标**上，就是模型在训练集上表现良好，但是在测试集和新数据上表现一般（**泛化能力差**）；

一是扩充数据集，并提高数据多样性，可用的方法如数据增强，增加数据等；
二是提前停止训练，在检测到过拟合时就提前将训练停止；
三是使用正则化，包括范数正则化和Dropout等；
四是削减模型的参数。

[算法岗常见面试题（三）：欠拟合和过拟合_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/469248235249336320?sourceSSR=users)

# 正则化的原理以及使用场景

你好，这是Bing。正则项是一种用于控制模型复杂度的技术，它可以防止模型过拟合或欠拟合，提高模型的泛化能力。常用的正则项有L1、L2和Lp，它们分别对应不同的范数（norm），即不同的度量方式。

L1正则项是指模型参数的绝对值之和，也称为L1范数或稀疏规则算子。它可以使模型参数变得稀疏，即许多参数为零，从而实现特征选择的效果。L1正则项的优点是可以减少特征的维度，提高模型的可解释性；缺点是可能会损失一些重要的特征，而且求解过程比较复杂。

L2正则项是指模型参数的平方和再开方，也称为L2范数或权重衰减。它可以使模型参数接近于零，但不会为零，从而减小模型对于极端数据的敏感性。L2正则项的优点是可以防止过拟合，提高模型的稳定性；缺点是不能实现特征选择，而且可能会导致欠拟合。

Lp正则项是指模型参数的p次方和再开p次方根，也称为Lp范数。它是L1和L2正则项的一般化形式，可以根据不同的p值调节模型的稀疏性和平滑性。当p=0时，Lp正则项等价于L0正则项，即模型参数中非零参数的个数；当p=1时，等价于L1正则项；当p=2时，等价于L2正则项；当p趋近于无穷时，等价于最大范数（max norm），即模型参数中绝对值最大的那个值。

loss正则化   [模型训练——Loss函数](https://zhuanlan.zhihu.com/p/436809988)
正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。[L1正则化与L2正则化](https://zhuanlan.zhihu.com/p/35356992)

pytorch实现 L2正则化 weight_decay用于设置权值衰减率，即正则化中的超参 ，默认值为0。
```python
e.g. optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.01)
```


（1） L1、L2区别  
L1是模型各个参数的绝对值之和。  
L2是模型各个参数的平方和的开方值。  
L1会趋向于产生少量的特征，而其他的特征都是0。  
因为最优的参数值很大概率出现在坐标轴上，这样就会导致某一维的权重为0 ，产生稀疏权重矩阵。  
L2会选择更多的特征，这些特征都会接近于0。  
最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化||w||时，就会使每一项趋近于0。  
（2） 优势

![](https://pic2.zhimg.com/80/v2-ef4f8e89e7be234d103b0cd1478ab5dd_720w.webp)

[Algorithm_Interview_Notes-Chinese/A-深度学习基础.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md#l1l2-%E8%8C%83%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96)

**为什么 L1 和 L2 正则化可以防止过拟合？**

-   L1 & L2 正则化会使模型偏好于更小的权值。
-   更小的权值意味着**更低的模型复杂度**；添加 L1 & L2 正则化相当于为模型添加了某种**先验**，限制了参数的分布，从而降低了模型的复杂度。
-   模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。——直观来说，就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声）——**奥卡姆剃刀原理**

个人总结：使模型偏好与权值较小的目标函数 -> 更低的模型复杂度

**L1/L2 范数的作用、异同**

**相同点**

-   限制模型的学习能力——通过限制参数的规模，使模型偏好于**权值较小**的目标函数，防止过拟合。

**不同点**

-   **L1 正则化**可以产生更**稀疏**的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；**L2 正则化**主要用于防止模型过拟合
-   **L1 正则化**适用于特征之间有关联的情况；**L2 正则化**适用于特征之间没有关联的情况。

**为什么 L1 正则化可以产生稀疏权值，而 L2 不会？**

-   对目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数 `J` 的最小值
    
-   带有**L1 范数**（左）和**L2 范数**（右）约束的二维图示
    
    [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png) [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png)
    
    -   图中 `J` 与 `L1` 首次相交的点即是最优解。`L1` 在和每个坐标轴相交的地方都会有“**顶点**”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 `J` 与这些“顶点”相交的机会远大于其他点，因此 `L1` 正则化会产生稀疏的解。
    -   `L2` 不会产生“**顶点**”，因此 `J` 与 `L2` 相交的点具有稀疏性的概率就会变得非常小。



[算法岗常见面试题（二）：正则化_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/468138806839926784?sourceSSR=users)

# 优化器

[算法岗常见面试题（六）：优化器_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/470889397320257536?sourceSSR=users)

Adam结合SGDM和AdaDelta两种优化算法的优点。对梯度的一阶动量（惯性）和二阶动量（更新频率）进行综合考虑，计算出更新步长。**一阶动量**的优势在于他能够学习到历史梯度下降的惯性，避免受到单个样本分布的干扰，**减少震荡**，加快收敛；**二阶动量**的优势在于是**自适应学习率**，为参数的不同维分配不同的学习率，在**模型稀疏的情况下效果很好**。

[Algorithm_Interview_Notes-Chinese/C-专题-优化算法.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/C-%E4%B8%93%E9%A2%98-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.md#%E4%B8%93%E9%A2%98-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)


# 几种卷积变体：空洞卷积、深度可分离卷积 因果卷积


因果卷积：
WAVENET中的扩张因果卷积，扩大感受野
如果考虑很久之前的变量x，就会导致卷积层数的增加。网络过于深会带来[梯度下降](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%25E6%25A2%25AF%25E5%25BA%25A6%25E4%25B8%258B%25E9%2599%258D%26spm%3D1001.2101.3001.7020)，训练复杂，拟合效果不好的问题，因此提出了空洞卷积。
[空洞卷积]([吃透空洞卷积(Dilated Convolutions) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/113285797))  增加感受野
在卷积核中增加空洞来增加感受野，不增加过多计算。普通卷积有着3*3的卷积核空洞卷积有着3*3的卷积核，空洞rate为2，可以使得神经网络在同样的层数下，拥有更大的感受野。  
空洞卷积存在的问题：  
空洞卷积的卷积核不连续，不是所有的信息参与了计算，导致信息连续性的损失，引起栅格效应。
[深度可分离卷积](https://zhuanlan.zhihu.com/p/166736637)   轻量级，**MobileNet**

# 因果性


因果性，只能用过去的信息
![image](https://cdn.staticaly.com/gh/andyye1999/picx-images-hosting@master/20230405/image.xospsy7s9lc.webp)


![image](https://cdn.staticaly.com/gh/andyye1999/picx-images-hosting@master/20230405/image.6r7tv8j6mo80.webp)


![image](https://cdn.staticaly.com/gh/andyye1999/picx-images-hosting@master/20230405/image.4klvixacxma0.webp)

# 感受野计算

[感受野]([感受野(Receptive Field)的理解与计算 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/113487374))

# CNN和RNN的区别和使用场景

CNN是卷积神经网络的简称，卷积神经网络一般由三个部分组成--输入层，输出层，和隐藏层。其核心的操作是卷积操作。卷积操作本质上是一种分组函数，CNN使用卷积来筛选数据并查找信息。卷积神经网络的特点是使用固定大小的输入和输出，并且学习的特征倾向于空间特征，因此卷积神经网络更适用于图像和视频的处理。RNN是循环神经网络的简称，与卷积神经网络不同的是，RNN中存在记忆单元，可以接受之前的信息指导当前的数据处理。循环神经网络的特点是可以接受变长的输出并产生变长的输出，并且学习的特征倾向于时间特征，或者说序列特征，因此循环神经网络更适用与语音和文本的处理。但是其实在具体领域这两者的应用没有具体分界，谁的性能好就用谁呗。


# 激活函数以及优缺点

[[深度学习中的激活函数]]



# self atention & transformer

[attention]([超详细图解Self-Attention - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/410776234))

[transformer]([熬了一晚上，我从零实现了Transformer模型，把代码讲给你听 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/411311520))



# 互相关




# 阵列幅度失真



幅度失真是因为时延估计和噪声协方差矩阵不准确造成的。而之所以不准确是因为，**时延估计和噪声协方差矩阵不准确**会造成**波束的主瓣方向出现偏差**，使**主瓣方向无法对准声源方向**，这样的话语音就会被抑制。


# PCM和WAV文件中存储的是啥

[[PCM WAV]]

# 采样率和采样深度的物理意义


采样位深，音频的位深度决定动态范围。我们常见的16Bit（16比特），可以记录大概96分贝的动态范围。那么，您可以大概知道，每一个比特大约可以记录6分贝的声音。同理，20Bit可记录的动态范围大概就是120dB；24Bit就大概是144dB。音频位速，也叫码率，或者比特率。位速是指在一个数据流中每秒钟能通过的信息量，也可以理解为：每秒钟用多少比特的数据量去表示。96kbps的WMA音频格式的音质明显要比96kbps的MP3音质好。为什么会这样呢？因为不同的压缩算法，对数据的利用率不同而造成的差异。再举例，假如MP3压缩至48kbps以下，已经惨不忍睹，而如果是AAC音频格式，同样是48kbps的位速下，音质明显比MP3好。


# 128点FFT变换，16kHz的音频，可以分为多少个子带？



# 维纳滤波和谱减法的区别



# 时延估计的实现



# WebRTC NS中的噪声估计用到的三个特征值是什么？它们分别的定义是什么？

[[webrtc_ns#FeatureUpdate() 提取平均LRT参数、频谱差异、频谱平坦度]]

# 你用的VAD的是做什么用的，原理是什么，遇到突发噪声怎么处理？

This is a Voice Activity Detection (VAD) code in C language. The code performs various operations to detect voice activity in an audio signal.

1.  FFT (Fast Fourier Transform) is performed on the input data buffer using the "r_fft" function with a flag value of +1.
    
2.  Energy in each channel is estimated by looping over the specified channels and computing the sum of squares of the real and imaginary parts of the FFT coefficients. The estimated energy is then smoothed using a constant value "CEE_SM_FAC".
    
3.  Channel noise is estimated for the first four frames by taking the maximum value between the channel energy and a constant "INE".通过取信道能量和常数“INE”之间的最大值来估计前四帧的信道噪声。
    
4.  The Signal-to-Noise Ratio (SNR) for each channel is calculated by dividing the channel energy by the channel noise and converting the result to dB.每个通道的信噪比 (SNR) 是通过将通道能量除以通道噪声并将结果转换为 dB 来计算的。
    
5.  The sum of voice metrics is calculated using a table "vm_tbl" that maps SNR values to voice metrics.使用将 SNR 值映射到语音指标的表“vm_tbl”计算语音指标的总和。
    
6.  The total noise estimate (tne) and total channel energy estimate (tce) are calculated by summing the channel noise and channel energy values respectively.总噪声估计 (tne) 和总信道能量估计 (tce) 分别通过对信道噪声和信道能量值求和来计算。
    
7.  Log spectral deviation is calculated by subtracting the long-term log spectral energy from the current log spectral energy and summing the absolute values.通过从当前对数光谱能量中减去长期对数光谱能量并将绝对值相加来计算对数光谱偏差。
    
8.  The long-term integration constant (alpha) is calculated based on the total channel energy estimate (tce). A higher total channel energy results in a slower integration and vice-versa.长期积分常数 (alpha) 是根据总信道能量估计 (tce) 计算的。 更高的总通道能量导致更慢的集成，反之亦然。
    
9.  Long-term log spectral energy is calculated by applying the integration constant to the current log spectral energy.通过将积分常数应用于当前对数光谱能量来计算长期对数光谱能量。
    
10.  The update flag is set or reset based on the sum of voice metrics and the total channel energy estimate.更新标志是根据语音度量和总信道能量估计的总和来设置或重置的。
    
11.  The modify flag is set or reset based on the current and previous update counts.
    

The purpose of the code is to determine whether an audio frame contains voice or non-voice content. The update and modify flags can be used to trigger additional processing such as speech enhancement, noise reduction, etc.


# 麦克风阵列的物理结构特性有哪些指标？你的麦克风阵列的指标都是怎样的？


麦克风阵列的物理结构特性的指标有以下几点：

1.  指向性：指麦克风阵列能够捕捉到的声音的方向性。一般情况下，麦克风阵列的指向性与麦克风的布局有关。
    
2.  分辨率：指麦克风阵列能够区分声音的位置的精度。麦克风阵列的分辨率取决于麦克风数量和布局。
    
3.  灵敏度：指麦克风阵列能够捕捉到声音的最小强度。灵敏度越高，麦克风阵列能够捕捉到的声音越弱。
    
4.  噪声级：指麦克风阵列自身产生的噪声。噪声级越低，麦克风阵列的性能越好。
    
5.  动态范围：指麦克风阵列能够捕捉到的声音的最大和最小强度差。动态范围越大，麦克风阵列的性能越好。
    
6.  过滤效果：指麦克风阵列能够滤掉外界杂音的效果。过滤效果越好，麦克风阵列的性能越好。
    
7.  抗干扰能力：指麦克风阵列在干扰环境下的工作效果。抗干扰能力越强，麦克风阵列的性能越好。

以上是由chatgpt回答


# 都用了哪些噪声种类？现在机器学习几百种噪声，为什么用这么少？



# 在哪些噪声环境下语音增强的效果比较差？



# 增强语音的结果的衡量标准是什么？


语音增强结果的衡量标准主要包括：

1.  听觉评价：根据人耳的听觉感受来评价语音的质量。例如 Mean Opinion Score (MOS)。
    
2.  信噪比：语音信号与噪声的比值。
    
3.  声音干净度：语音信号中的噪声减少了多少。
    
4.  语音信息内容保留度：增强语音信号与原始语音信号的相似度。
    
5.  语音特征保留度：语音增强后语音特征的保留情况，例如语音语调、语速、音高、音色等。

不同的语音增强任务可能需要不同的衡量标准，对于每一种语音增强算法，其衡量标准通常要结合其具体任务和应用场景进行评估。
以上由chatgpt进行回答

ASR  

PESQ STOI MOS  SISNR   DNSMOS参赛队伍才有资格
带宽扩展中有log-spectral distance (LSD) high-frequency log-spectral distance (LSD-HF) 

**如何判断噪声估计是否准，可以加入均值方差已知的高斯白噪声来看估计的噪声准不准**

#  k-means原理及实现（会不会出现一类没有信息

**（1）原理**

K-means算法是最常用的一种[聚类算法](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%25E8%2581%259A%25E7%25B1%25BB%25E7%25AE%2597%25E6%25B3%2595%26spm%3D1001.2101.3001.7020)。算法的输入为一个样本集（或者称为点集），通过该算法可以将样本进行聚类，具有相似特征的样本聚为一类。针对每个点，计算这个点距离所有中心点最近的那个中心点，然后将这个点归为这个中心点代表的簇。一次迭代结束之后，针对每个簇类，重新计算中心点，然后针对每个点，重新寻找距离自己最近的中心点。如此循环，直到前后两次迭代的簇类没有变化。  
下面通过一个简单的例子，说明K-means算法的过程。如下图所示，目标是将样本点聚类成3个类别。

![](https://pic4.zhimg.com/80/v2-4cb4c9faeb6021f862ac3f9add371547_720w.webp)

**（2）算法流程**

选择聚类的个数k（kmeans算法传递超参数的时候，只需设置最大的K值）  
任意产生k个聚类，然后确定聚类中心，或者直接生成k个中心。  
对每个点确定其聚类中心点。  
再计算其聚类新中心。  
重复以上步骤直到满足收敛要求。（通常就是确定的中心点不再改变。）  
**上述步骤的关键两点是：**找到距离自己最近的中心点。更新中心点。

**（3）python实现**

```python
# K-means Algorithm is a clustering algorithm
import numpy as np
import matplotlib.pyplot as plt
import random
 
 
def get_distance(p1, p2):
    diff = [x-y for x, y in zip(p1, p2)]
    distance = np.sqrt(sum(map(lambda x: x**2, diff)))
    return distance
 
 
# 计算多个点的中心
# cluster = [[1,2,3], [-2,1,2], [9, 0 ,4], [2,10,4]]
def calc_center_point(cluster):
    N = len(cluster)
    m = np.matrix(cluster).transpose().tolist()
    center_point = [sum(x)/N for x in m]
    return center_point
 
 
# 检查两个点是否有差别
def check_center_diff(center, new_center):
    n = len(center)
    for c, nc in zip(center, new_center):
        if c != nc:
            return False
    return True
 
 
# K-means算法的实现
def K_means(points, center_points):
 
    N = len(points)         # 样本个数
    n = len(points[0])      # 单个样本的维度
    k = len(center_points)  # k值大小
 
    tot = 0
    while True:             # 迭代
        temp_center_points = [] # 记录中心点
 
        clusters = []       # 记录聚类的结果
        for c in range(0, k):
            clusters.append([]) # 初始化
 
        # 针对每个点，寻找距离其最近的中心点（寻找组织）
        for i, data in enumerate(points):
            distances = []
            for center_point in center_points:
                distances.append(get_distance(data, center_point))
            index = distances.index(min(distances)) # 找到最小的距离的那个中心点的索引，
 
            clusters[index].append(data)    # 那么这个中心点代表的簇，里面增加一个样本
 
        tot += 1
        print(tot, '次迭代   ', clusters)
        k = len(clusters)
        colors = ['r.', 'g.', 'b.', 'k.', 'y.']  # 颜色和点的样式
        for i, cluster in enumerate(clusters):
            data = np.array(cluster)
            data_x = [x[0] for x in data]
            data_y = [x[1] for x in data]
            plt.subplot(2, 3, tot)
            plt.plot(data_x, data_y, colors[i])
            plt.axis([0, 1000, 0, 1000])
 
        # 重新计算中心点（该步骤可以与下面判断中心点是否发生变化这个步骤，调换顺序）
        for cluster in clusters:
            temp_center_points.append(calc_center_point(cluster))
 
        # 在计算中心点的时候，需要将原来的中心点算进去
        for j in range(0, k):
            if len(clusters[j]) == 0:
                temp_center_points[j] = center_points[j]
 
        # 判断中心点是否发生变化：即，判断聚类前后样本的类别是否发生变化
        for c, nc in zip(center_points, temp_center_points):
            if not check_center_diff(c, nc):
                center_points = temp_center_points[:]   # 复制一份
                break
        else:   # 如果没有变化，那么退出迭代，聚类结束
            break
 
    plt.show()
    return clusters # 返回聚类的结果
 
# 随机获取一个样本集，用于测试K-means算法
def get_test_data():
 
    N = 1000
 
    # 产生点的区域
    area_1 = [0, N / 4, N / 4, N / 2]
    area_2 = [N / 2, 3 * N / 4, 0, N / 4]
    area_3 = [N / 4, N / 2, N / 2, 3 * N / 4]
    area_4 = [3 * N / 4, N, 3 * N / 4, N]
    area_5 = [3 * N / 4, N, N / 4, N / 2]
 
    areas = [area_1, area_2, area_3, area_4, area_5]
    k = len(areas)
 
    # 在各个区域内，随机产生一些点
    points = []
    for area in areas:
        rnd_num_of_points = random.randint(50, 200)
        for r in range(0, rnd_num_of_points):
            rnd_add = random.randint(0, 100)
            rnd_x = random.randint(area[0] + rnd_add, area[1] - rnd_add)
            rnd_y = random.randint(area[2], area[3] - rnd_add)
            points.append([rnd_x, rnd_y])
 
    # 自定义中心点，目标聚类个数为5，因此选定5个中心点
    center_points = [[0, 250], [500, 500], [500, 250], [500, 250], [500, 750]]
 
    return points, center_points
 
 
if __name__ == '__main__':
 
    points, center_points = get_test_data()
    clusters = K_means(points, center_points)
    print('#######最终结果##########')
    for i, cluster in enumerate(clusters):
        print('cluster ', i, ' ', cluster)
```

**（4）会不会出现一类空？**

会，如果两类之间距离太近





