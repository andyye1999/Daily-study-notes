

# 直线麦克风阵列怎么分左右

面阵，十字阵才能分

# 频率混叠 栅栏效应

如果不满足采样定理，则会发生频率折叠现象。

采样定理告诉我们，对于一个最高频率为 $f_{\text{max}}$ 的连续信号，其最高可以采样频率为 $2f_{\text{max}}$。如果采样频率小于 $2f_{\text{max}}$，则会发生频率折叠。

频率折叠的现象是：在频域中的高频部分会折叠到低频部分，造成频率冗余和信号失真。因此，我们在采样信号时必须遵循采样定理，以确保采样后的信号能够准确反映原始信号的频率特征。以上内容由chatgpt回答

栅栏效应是因为DFT计算的频谱被限制在基频的整数倍而不可能将频谱视为一个连续函数而产生的。就一定意义而言，栅栏效应表现为用DFT计算整个频谱时，就好像通过一个“栅栏”来观看一个图景一样，只能在离散点的地方看到真实图景。

增加频域抽样点数N，同时在不改变时域数据的情况下，在时域数据末端添加一些零值点，使得谱线更密，这样就可以减小栅栏效应，观察到原来看不到的频谱分量。注意，该方法通过补零来增加N，此时采样频率f(s)会随之成正比上升，又由于频率分辨率F=f(s)/N，频率分辨率不改变，也就是说，补零不改变频率分辨率。

看这个
**[[语音信号基础知识#频谱泄露和混叠]]**

# 抽取与插值

[[信号的抽取与插值与子带滤波器组]]

抽样频率fs降到1/M fs  
先低通滤波，再抽取 为了满足抽样定理  

fs 增加L倍  
先插值，再滤波   低通滤波 fs/2
# RNN GRU LSTM

[[深度学习#RNN]]
GRU 认为 LSTM 中的**遗忘门**和**输入门**的功能有一定的重合，于是将其合并为一个**更新门**
- GRU 把遗忘门和输入门合并为**更新门（update）** `z`，并使用**重置门（reset）** `r` 代替输出门；
- **合并**了记忆状态 `C` 和隐藏状态 `h`

LSTM 是如何实现长短期记忆的？（遗忘门和输入门的作用）

-   LSTM 主要通过**遗忘门**和**输入门**来实现长短期记忆。
    -   如果当前时间点的状态中没有重要信息，遗忘门 `f` 中各分量的值将接近 1（`f -> 1`）；输入门 `i` 中各分量的值将接近 0（`i -> 0`）；此时过去的记忆将会被保存，从而实现**长期记忆**；
    -   如果当前时间点的状态中出现了重要信息，且之前的记忆不再重要，则 `f -> 0`，`i -> 1`；此时过去的记忆被遗忘，新的重要信息被保存，从而实现**短期记忆**；
    -   如果当前时间点的状态中出现了重要信息，但旧的记忆也很重要，则 `f -> 1`，`i -> 1`。
# RNN如何解决梯度问题

[Algorithm_Interview_Notes-Chinese/B-专题-RNN.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/B-%E4%B8%93%E9%A2%98-RNN.md#rnn-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8)


# FIR IIR


程序编写[[IIR和FIR滤波器代码]]


滤波器可分为两种，IIR（无限冲激响应）滤波器和FIR（有限冲激响应）滤波器。

**（1）FIR和IIR滤波器的不同**

FIR滤波器的冲激响应在有限时间内衰减为0，输出仅取决于当前和过去的输入信号值，在Z域上其极点位置只能是原点，而IIR滤波器的冲激响应会无限持续，输出不仅取决于当前和过去的输入信号，还和过去的输出有关，IIR的极点可以处于单位圆内任何地方。  
设计同样参数的滤波器，FIR要比IIR需要更多的参数，也就是在处理时需要更长的时间去计算，实时性差一些。  
FIR具有线性相位，IIR不具有，非线性相位是指对于不同的频率分量造成的相位差与频率不成比例，使得输出时不同频率分量的叠加的相位值和输入时有变化，从而导致了信号的失真。因此在进行IIR设计的时候需考虑这些，如有相位要求需添加相位校准网络。  
在实际应用中，如果滤波器通带内不要求线性相位，则使用IIR，若有要求，则根据相位失真度、计算量、复杂度等因素综合考虑是选择FIR还是选择IIR+相位补偿。

**FIR滤波器和IIR滤波器的区别**

（1）在相同技术指标下，IIR滤波器由于存在着输出对输入的反馈，IIR滤波阶数比FIR滤波器阶数小；

（2）FIR滤波器可得到严格的线性相位，IIR没有；（IIR滤波器要满足线性相位和幅度滤波要求，必须加全通网络进行相位校正，会大大增加滤波器阶数）；

（3）FIR采用非递归结构，所以较为稳定（极点固定在原点）；IIR滤波器必须采用递归结构，极点必须在单位圆内才稳定（极点在z平面任意位置）；

（4）FIR滤波器冲击响应是有限长，可以使用FFT计算；IIR不可以；

（5）设计IIR滤波器可利用设计模拟滤波器现成的公式，计算量较小；FIR只能通过计算机计算设计；

（6）IIR只能设计规格化的、频率特性为低通、高通、带通、带阻、全通滤波器；FIR滤波器可适应各种幅度特性的要求，更为灵活。

**（2）FIR和IIR设计方法**

1FIR：窗函数法、频率采样法、切比雪夫逼近法。对比这三种方法，窗函数法是最早提出的，缺少关键频率的精度控制，如用该种方法设计的低通滤波器，它的截止频率依赖于窗函数的类型和滤波器长度M，并不能从截止频率出发进行设计。频率采样法指定了一些w处H（w）的值，并规定了过渡带为2π/M的倍数，由于这种H（w）在过渡带以外的其他频率上为0或1的曲线特性，这种方法用于实现频域滤波。切比雪夫逼近法在技术指标的管控上比前两种都要好，可以按照参数wp，ws，δ1，δ2，M给定的技术指标，指定参数wp，ws，δ1，M，然后利用δ2来优化滤波器，这种方法后续可着重研究一下。  
IIR：由于模拟滤波器是一个充分研究的成熟领域，多使用模拟滤波器转换到数字滤波器上。导数逼近法、冲击不变法、双线性变换法。前两种方法有严重的局限性，仅适合于低通滤波器和一类有限的带通滤波器，双线性变换法则无此限制。常用的模拟滤波器有巴特沃斯滤波器、切比雪夫滤波器、椭圆滤波器、贝塞尔滤波器。

# 线性相位

相位谱一条直线

纯延时  

相位失真

# FIR滤波器和IIR滤波器的设计

窗函数法设计FIR滤波器

（1）给定所要求的理想频率响应函数$H_d(e^{jw})$，求出反变换$h_d(n)$；

（2）由过度带宽及阻带最小衰减的要求，选定窗函数的形状及N的大小，由此可以得到窗函数的表达式；

（3）求所设计的FIR滤波器的单位冲击响应$h(n)=h_d(n)w(n)$，做FFT得到$H(e^(jw))$；

设计IIR滤波器

（1）根据所要设计滤波器的参数去确定一个模拟滤波器的传输函数；

（2）再根据巴特沃斯型、切比雪夫型等滤波器去模拟低通滤波器；

（3）通过双线性变换、或冲击响应不变法来进行数字滤波器的设计。

![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230803/image.3d1ce4bw3fs0.webp)

# 双线性变换和冲击响应不变法的优缺点

冲击响应不变法

- 优点：模拟频率到数字频率的转换是线性的；
- 缺点：会产生频谱混叠现象，只适合带限滤波器；

双线性变换

- 优点：克服多值映射关系，可以消除频率的混叠；
- 缺点：模拟频率到数字频率的转换是非线性的，在高频由较大的失真；



# 什么是线性相位

线性相位能保证信号中各频率成分的相对相位关系不改变。相频响应是一条直线

通俗解释是：信号经过滤波器后，各个频率分量的延时时间是一样的。

# FT、DTFT、DFT和DFS

傅里叶级数对应的是周期信号
傅里叶变换对应的是非周期信号

1. 对一个长度为N的有限长序列进行DTFT运算后，再把得到的频谱进行抽样频率为ωs=2π/N 的抽样，抽样后的结果等于对这个序列进行DFT运算的结果；
2. 将长度为N的有限长序列做周期为N的周期延拓后，再进行DFS运算，就是对原有限长序列进行DFT运算。
DFS是对周期序列的变换，DFT是对有限长序列的变换

DTFT是对离散非周期

![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230803/image.5m4zyjssda80.webp)


# 重采样

[[信号的抽取与插值与子带滤波器组]]
[[语音基础/重采样]]

多相滤波结构
# MFCC

[[MFCC]]


![](https://pic1.zhimg.com/80/v2-2bd74388bcd020fbaeff2a99e24ef800_720w.webp)


MFCC和FBANK MFCC在FBANK做DCT ，**DCT是线性变换，容易损失FBANK非线性信息。DCT相当于去相关 FBANK相关性高（相邻滤波器组有重叠）MFCC具有更好的判别度，适合做特征。MFCC计算量大**

为什么取13阶MFCC
选择前13个MFCC系数的原因是：

-   [经验表明，少于13个系数会影响识别性能，多于13个系数对性能的提高不明显](https://www.zhihu.com/question/24502675)[1](https://www.zhihu.com/question/24502675)。
-   [13个系数是一种习惯，也可以选择12或14](https://www.zhihu.com/question/24502675)[1](https://www.zhihu.com/question/24502675)[2](https://blog.csdn.net/weixin_42788078/article/details/101422144)。
-   [DCT的作用是获得频谱的倒谱，倒谱的低频分量就是频谱的包络，倒谱的高频分量就是频谱的细节，这些都是语音识别上有效的语音物理信息](https://blog.csdn.net/weixin_42788078/article/details/101422144)[2](https://blog.csdn.net/weixin_42788078/article/details/101422144)[3](https://www.zhihu.com/question/24502690)。
-   [为了平衡识别准确度和计算速度，以及去除不需要的信息](https://www.zhihu.com/question/24502690)[3](https://www.zhihu.com/question/24502690)。

# 频率分辨率

![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20230107/image.rns6p7jmqxs.webp)

![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20230107/image.1etnrunmea8.webp)

![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20230107/image.3j2cww9uekm0.webp)


![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20230107/image.359syy3mfym0.webp)

频率分辨率与反比于**模拟信号的长度T**

时间分辨率与频率分辨率相互制约，可以用矩形窗和sinc函数解释，矩形窗越大，sinc越接近脉冲
计算分辨率计算时fs是不变的，但补零会导致fs变大，此时采样频率f(s)会随之成正比上升，又由于频率分辨率F=f(s)/N，频率分辨率不改变，也就是说，补零不改变频率分辨率。

# 相关与卷积的区别与联系

![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20230107/image.701o64broe00.webp)

# 卷积C语言

下面是一个简单的C语言卷积函数实现，假设输入数组`x`的大小为`N`，卷积核数组`h`的大小为`M`，输出数组`y`的大小为`N+M-1`：

```c
void convolution(double x[], double h[], double y[], int N, int M) {
    int i, j, k;
    for (i = 0; i < N + M - 1; i++) {
        y[i] = 0;
        for (j = 0; j < M; j++) {
            k = i - j;
            if (k >= 0 && k < N) {
                y[i] += x[k] * h[j];
            }
        }
    }
}
```

# 圆周卷积和线性卷积

[[线性卷积圆周卷积]]

**线性卷积（相关）和圆周卷积（相关）之间的关系**

1.  一般的，如果两个有限长序列的长度为N1和N2，且满足N1≥N2，则有**圆周卷积**的**后** N1−N2+1个点，与**线性卷积**的结果一致。
2.  一般的，如果两个有限长序列的长度为N1和N2，且满足N1≥N2，则有**圆周相关**的**前** N1−N2+1个点，与**线性相关**的结果一致。
3.  时域中的**圆周卷积**对应于其**离散傅里叶变换的乘积**
4.  时域中的**圆周相关**对应于其**离散傅里叶变换共轭谱的乘积**

我们这里以overlap save method为例，为了确保能得到N个点的线性卷积输出信号，我们至少要保证有N个点的线性卷积和圆周卷积的结果一致（预备知识）

N1−N2+1=N

由于N1≥N2 (输入信号长度通常大于滤波器的阶数)，且N2=N (滤波器的阶数为N)，那么要求每次参与运算的输入信号长度N1至少为2N−1，为了计算FFT方便，我们令输入信号的长度为：N1=2N，那么我们FFT的长度也为2N

为了构造长度为2N的数据，我们需要在每个N阶滤波器后面N补零

![](https://img2020.cnblogs.com/blog/1433301/202012/1433301-20201227172619795-855301426.png)

要求线性卷积(预备知识1)，我们就需要 求圆周卷积后N1−N2+1个点，根据预备知识3，我们只需要求 离散傅里叶变换的乘积 就能得到圆周卷积的结果，接下来我们分别计算 输入信号向量 和 滤波器系数向量 的FFT：

# 为什么时域卷积等于频域乘积

时域信号可以分解成一串不同频率正弦信号的叠加。根据卷积的分配率，两个时域信号的卷积最终可以展开成两两正弦信号的卷积的和。由于不同频率的正弦信号的卷积为0，所以最终只剩下相同频率的正弦信号的卷积。而卷积的结果就是频率不变，幅度相乘。

在频域里边就表现为直接相乘。

# 预加重

[[预加重]]  预加重 一阶高通滤波器 **语音能量集中在低频部分，造成语音信号高频端信噪比可能低 预加重增加语音高频分量，之后去加重减小噪声高频分量**
1-0.8z^-1 

# 加窗 不同窗特点


[[语音信号基础知识#加窗]]

**主瓣宽度越窄越好，旁瓣衰减越大越好**

对连续的语音分帧做STFT处理，等价于截断一段时间信号，对其进行周期性延拓，从而变成无限长序列，并对该无限长序列做FFT变换，这一截断并不符合傅里叶变换的定义。因此，会导致频谱泄露和混叠

频谱泄露会导致幅度较小的频点淹没在幅度较大的频点泄露分量中，

而混叠会在分段拼接处引入虚假的峰值，进而不能获得准确的频谱情况

加窗是为了抑制频谱泄露和混叠的产生

不同的窗函数有不同的特点。一般来说，选择窗函数要考虑的因素有：

1.  带噪比：窗函数的形状直接影响语音分帧的带噪比，比如矩形窗带来的噪音大于汉宁窗。
    
2.  稳定性：窗函数的形状直接影响语音分帧的稳定性，比如汉宁窗与语音信号更加稳定。
    
3.  计算复杂度：不同的窗函数具有不同的计算复杂度，例如汉宁窗与矩形窗。
    

常见的窗函数包括：矩形窗、汉宁窗、Hann窗、Hamming窗、Blackman窗等。每种窗函数的适用场景都不同，取决于应用需求。以上不同窗特点为chatGPT回答

# istft 完美重构的条件是什么

OLA时 窗和为1 能量

# 自适应滤波

[[FDAF自适应滤波器算法综述]]

# 推导FFT

看书

# 实数FFT

里面有IFFT的程序如何计算
FFT公式以及如何计算实数FFT

[[fft与实数fft]]

IFFT程序步骤

1. 将X(K)取共轭
2. 直接做FFT
3. 对FFT结果取共轭并乘以1/N

# FFT复杂度

![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20230108/image.35gw6tlg3980.webp)

分子DFT复杂度 N方复数乘法 N(N-1)复数加法 一次复数乘法等于四次实数乘两次实数加


# 傅里叶变换的四种解释

傅里叶 矩阵 相关 滤波器 

# 音乐噪声


[[语音增强理论与实践#音乐噪声]]

# 定点化

定点化中SQRT  牛顿迭代法 或者二分法   和 log是怎么实现的  查表法 非线性运算 （指令 查表法  混合法）

定点化基本上在50多db左右符合  最好能60-70db

[[定点化]] [[定点化基础.pdf]]

# 算法层面的优化

C语言层面的优化  汇编层优化  芯片层优化

优化的分类？  一个是循环层面  去除多于循环 第二个是嵌套的展开 另一个层面是指令集的操作 包括汇编等指令   
循环中 两层for循环的优化：  尽量内层比外层大 用++i而不是i++ 避免在for循环的括号内计算
# 随机信号

![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230803/image.66zzf23f7yo0.webp)

![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230803/image.3d1ce4bw3fs0.webp)

宽平稳  均值 方差 常数  自相关函数与n1,n2的选取点无关，只与差有关

白噪声： 其功率谱始终为常数 自相关函数如上图所示

马尔可夫链 已知在现在的时刻tn的状态为X(N)，那么下一时刻tn+1的状态X(N+1)只和现在的状态有关，和过去的状态X(N-1),...X(0)无关


# 希尔伯特变换

90°相移器 f(t) * 1 / (PI * t )
# 如何理解去直流和直流分量

音频信号的均值代表音频信号的直流分量 消除偏置？
没什么用，所以不管它，比如提取特征时就不管这个维度


# 人耳对声音的敏感范围  人耳的能够听到的频率范围

人耳对声音的敏感范围：1k-8kHz；人耳的能够听到的频率范围：20Hz-20kHz

# 损失函数的设计思路，以及常见的损失函数

  
[模型训练——Loss函数](https://zhuanlan.zhihu.com/p/436809988)

[[深度学习中的激活函数]]


[Algorithm_Interview_Notes-Chinese/A-深度学习基础.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)


 **`ReLU` 相比 `sigmoid` 的优势 (3)**

1.  **避免梯度消失**
    -   `sigmoid`函数在输入取绝对值非常大的正值或负值时会出现**饱和**现象——在图像上表现为变得很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失；
    -   `ReLU` 的导数始终是一个常数——负半区为 0，正半区为 1——所以不会发生梯度消失现象
2.  **减缓过拟合**
    -   `ReLU` 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——**稀疏激活**
    -   这有助于减少参数的相互依赖，缓解过拟合问题的发生
3.  **加速计算***
    -   `ReLU` 的求导不涉及浮点运算，所以速度更快


**sidir**    [[conv-tasnet#si-snr]]


多分辨率stftloss 各种time和freq的组合

# 梯度爆炸和梯度消失问题的成因和缓解

梯度爆炸的成因： 在深度学习中，梯度爆炸是指在计算损失函数的梯度时，因某些原因使得梯度值变得非常大，从而导致训练无法正常进行。通常发生在链式求导时，因为每层的梯度都是从上一层传递下来的，当上一层的梯度值很大时，就可能导致梯度爆炸。

梯度消失的成因： 梯度消失是指在计算损失函数的梯度时，因某些原因使得梯度值变得非常小，从而导致训练无法正常进行。通常发生在使用非线性激活函数（如sigmoid）的网络中，当激活函数的输入较大时，激活函数的导数很小，从而导致梯度值很小。

缓解梯度爆炸和梯度消失的方法：

-   使用更多的Batch Normalization
-   使用更大的学习率
-   使用更小的模型
-   在链式求导的过程中使用更强的正则化方法，如Dropout
-   使用更复杂的激活函数，如ReLU
-   使用更好的优化器，如Adam


（1）梯度裁剪  
梯度裁剪主要是针对梯度爆炸提出。其思想也比较简单，训练时候设置一个阈值，梯度更新的时候，如果梯度超过阈值，那么就将梯度强制限制在该范围内，这时可以防止梯度爆炸。  
权重正则化（weithts regularization)也可以解决梯度爆炸的问题，其思想就是我们常见的正则方式。

![](https://pic4.zhimg.com/80/v2-d7636acbe4530c258747b76cd49fa74f_720w.webp)

α是正则化系数。如果发生梯度爆炸，||w||的平方会变得非常大，这样就可以一定程度避免梯度爆炸。  
（2）relu等激活函数  
relu我们就非常常见了，在AlexNet网络中最先提出。relu激活函数的导数为1，那么就不存在梯度消失爆炸的问题，不同层之间的梯度基本保持一致。  
而relu的缺点则是，负数部分恒为0，所以存在一定‘死区’，会导致一些神经元无法被激活，可以通过elu等来改善死区的问题。  
（3）batch normalization  
batch normalization目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果.BN本质上是解决反向传播过程中的梯度问题，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。  
在我们前面推导的反向传播求导公式中，含有w项，所以w的大小影响了梯度的消失和爆炸。BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。  
（4）ResNet 残差结构  
Residual Net中包含残差的shortcut(捷径)部分，shortcut的网络结构为

![](https://pic2.zhimg.com/80/v2-0d05ff3e2afe662a9d38f011ecaa01f5_720w.webp)

残差网络主要是为了解决梯度消失的问题。从上面的网络结构可以看出，由于shortcut的存在，残差网络的输出在对于输入求导时，总有一个x保证有一个常数梯度1(除非F(X)刚好求导为-1这样导数求和为0，但这种概率太小)，所以一定程度能解决梯度消失的问题。如果从输入到输出，恒等映射是最优解，那么将残差F(x)直接设置为0即可。
以上由chatgpt回答

[算法岗常见面试题（五）：梯度消失和梯度爆炸_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/470250150548545536?sourceSSR=users)

防止梯度爆炸：

1.  梯度剪切：更新梯度时，梯度超过某个阈值，就将其强制限制在这个范围内
2.  权重正则化：L1正则和L2正则

防止梯度消失：

1.  合理的激活函数（如ReLU）+权重初始化
2.  Batch Normalization：应用于每层激活函数之前
3.  残差网络

# 几种不同的归一化方式的区别和联系

归一化是一种常见的数据预处理方法，目的是将数据转换为固定范围内的数值。常用的几种归一化方式有以下几种：

1.  Min-Max 归一化：这种归一化方法通过将数据的最大值和最小值缩放到0-1的范围内。公式为：x' = (x-x_min)/(x_max-x_min)
    
2.  Z-Score 归一化：这种归一化方法通过计算数据的均值和标准差，将数据标准化为均值为0，标准差为1的正态分布数据。公式为：x' = (x-μ)/σ
    
3.  小数定标归一化：这种归一化方法通过计算数据的最大的整数位数，将数据的最大值归一化为1，其他数据相对地缩放。
    
4.  极差归一化：这种归一化方法通过计算数据的极差，将数据缩放到0-1的范围内。公式为：x' = (x-x_min)/(x_max-x_min)


以上四种归一化方式各有优劣，具体选择哪种归一化方式取决于数据的特征、数据处理目的以及其他因素。此外，不同归一化方式在模型训练时可能会影响模型的训练结果，因此需要根据具体场景进行选择。

以上由chatgpt回答

# 音量的归一化


避免非因果 递归归一化
![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230405/image.35vrfxh4i5u0.webp)


或者在损失函数归一化
![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230405/image.2xq54ule4cs0.webp)

# dropout的原理，训练和推理的不同

Dropout是一种防止神经网络过拟合的技术，它的原理是在训练过程中随机丢弃一部分神经元，从而减少神经元之间的共适应关系，增强网络的泛化能力。

训练和推理的不同在于，训练时每个神经元以一定的概率p被置为0，而推理时所有的神经元都是激活的，但是要对输出进行缩放，乘以1-p或者p，以保持输出的期望不变。

训练时按照概率P丢弃（特征置0）其实就是让它的激活函数值以概率p变为0. 测试时所有特征xP

[深度学习中Dropout原理解析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/38200980)

Dropout层的位置

Dropout一般放在**全连接层**防止过拟合，提高模型返回能力，由于卷积层参数较少，很少有放在卷积层后面的情况，卷积层一般使用batch norm。  
全连接层中一般放在激活函数层之后


# batchnorm

[Algorithm_Interview_Notes-Chinese/A-深度学习基础.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md#batch-normalization%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96)

- BN 是一种**正则化**方法（减少泛化误差），主要作用有：
    - **加速网络的训练**（缓解梯度消失，支持更大的学习率）
    - **防止过拟合**
    - 降低了**参数初始化**的要求。

- **训练的本质是学习数据分布**。如果训练数据与测试数据的分布不同会**降低**模型的**泛化能力**。因此，应该在开始训练前对所有输入数据做归一化处理。
- 而在神经网络中，因为**每个隐层**的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生改变；**致使**网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与**过拟合**的风险。


BN 方法会针对**每一批数据**，在**网络的每一层输入**之前增加**归一化**处理，使输入的均值为 `0`，标准差为 `1`。**目的**是将数据限制在统一的分布下。

具体来说，针对每层的第 `k` 个神经元，计算**这一批数据**在第 `k` 个神经元的均值与标准差，然后将归一化后的值作为该神经元的激活值。  
  [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180831165546.png)](http://www.codecogs.com/eqnedit.php?latex=\fn_jvn&space;\large&space;\hat{x}_k\leftarrow&space;\frac{x_k-\mathrm{E}[x_k]&space;}{\sqrt{\mathrm{Var}[x_k]}})

BN 可以看作在各层之间加入了一个新的计算层，**对数据分布进行额外的约束**，从而增强模型的泛化能力；
但同时 BN 也降低了模型的拟合能力，破坏了之前学到的**特征分布**；
为了**恢复数据的原始分布**，BN 引入了一个**重构变换**来还原最优的输入数据分布  [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180831165516.png)](http://www.codecogs.com/eqnedit.php?latex=\fn_jvn&space;\large&space;y_k\leftarrow&space;\gamma&space;\hat{x}_k+\beta)
 其中 `γ` 和 `β` 为可训练参数。

以上过程可归纳为一个 **`BN(x)` 函数**：
    [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903223427.png)](http://www.codecogs.com/eqnedit.php?latex=\large&space;\boldsymbol{y_i}=\mathrm{BN}(\boldsymbol{x_i}))

其中

[![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903224323.png)](http://www.codecogs.com/eqnedit.php?latex=\large&space;\begin{aligned}&space;\mathrm{BN}(\boldsymbol{x_i})&=\gamma\boldsymbol{\hat{x}_i}+\beta\\&space;&=\gamma\frac{\boldsymbol{x_i}-\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]}+\epsilon}}+\beta&space;\end{aligned})

**完整算法**： 
[![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180903222433.png)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/_assets/TIM%E6%88%AA%E5%9B%BE20180903222433.png)


# batchnorm 训练和测试时不同 

- **训练时**每次会传入一批数据，做法如前述；
    
- 当**测试**或**预测时**，每次可能只会传入**单个数据**，此时模型会使用**全局统计量**代替批统计量；
    
    - 训练每个 batch 时，都会得到一组`（均值，方差）`；
        
    - 所谓**全局统计量，就是对这些均值和方差求其对应的数学期望；**
        
    - 具体计算公式为：
        
        [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903220828.png)](http://www.codecogs.com/eqnedit.php?latex=\fn_jvn&space;\large&space;y_k\leftarrow&space;\gamma&space;\hat{x}_k+\beta)
        

其中 `μ_i` 和 `σ_i` 分别表示第 i 轮 batch 保存的均值和标准差；`m` 为 batch_size，系数 `m/(m-1)` 用于计算**无偏方差估计**
原文称该方法为**移动平均**（moving averages）

- 此时，`BN(x)` 调整为：
    
    [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/%E5%85%AC%E5%BC%8F_20180903224557.png)](http://www.codecogs.com/eqnedit.php?latex=\large&space;\begin{aligned}&space;\mathrm{BN}(\boldsymbol{x_i})&=\gamma\frac{\boldsymbol{x_i}-\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]}+\epsilon}}+\beta\\&space;&=\frac{\gamma}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]}+\epsilon}}\boldsymbol{x_i}+\left&space;(&space;\beta-\frac{\gamma\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]}+\epsilon}}&space;\right&space;)&space;\end{aligned})

**完整算法**：

[![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180903224842.png)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/_assets/TIM%E6%88%AA%E5%9B%BE20180903224842.png)

[pytorch BatchNorm参数详解，计算过程 - 水木清扬 - 博客园 (cnblogs.com)](https://www.cnblogs.com/shuimuqingyang/p/14007260.html)


# batchnorm  layernorm istancenorm

DPCRN 模型中使用即时层归一化 (iLN)，InstanceNorm2d 而不是普通的 LN，其中所有帧在频率轴 f 和通道轴 c 上独立计算统计数据，并共享相同的可训练参数

具体差别看[深度学习中的Normalization方法 - 凌逆战 - 博客园 (cnblogs.com)](https://www.cnblogs.com/lxp-never/p/11566064.html#blogTitle5)

batchnorm 是在batch维度 layernorm是将channel w h 进行归一化  istancenorm是对w h 进行归一化

istancenorm 流式推理时 需要将track_running_stats 设置为true

# 过拟合与欠拟合

-   **欠拟合**指模型不能在**训练集**上获得足够低的**训练误差**；
-   **过拟合**指模型的**训练误差**与**测试误差**（泛化误差）之间差距过大；反映在**评价指标**上，就是模型在训练集上表现良好，但是在测试集和新数据上表现一般（**泛化能力差**）；

一是扩充数据集，并提高数据多样性，可用的方法如数据增强，增加数据等；
二是提前停止训练，在检测到过拟合时就提前将训练停止；
三是使用正则化，包括范数正则化和Dropout等；
四是削减模型的参数。

[算法岗常见面试题（三）：欠拟合和过拟合_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/469248235249336320?sourceSSR=users)

# 正则化的原理以及使用场景

你好，这是Bing。正则项是一种用于控制模型复杂度的技术，它可以防止模型过拟合或欠拟合，提高模型的泛化能力。常用的正则项有L1、L2和Lp，它们分别对应不同的范数（norm），即不同的度量方式。

L1正则项是指模型参数的绝对值之和，也称为L1范数或稀疏规则算子。它可以使模型参数变得稀疏，即许多参数为零，从而实现特征选择的效果。L1正则项的优点是可以减少特征的维度，提高模型的可解释性；缺点是可能会损失一些重要的特征，而且求解过程比较复杂。

L2正则项是指模型参数的平方和再开方，也称为L2范数或权重衰减。它可以使模型参数接近于零，但不会为零，从而减小模型对于极端数据的敏感性。L2正则项的优点是可以防止过拟合，提高模型的稳定性；缺点是不能实现特征选择，而且可能会导致欠拟合。

Lp正则项是指模型参数的p次方和再开p次方根，也称为Lp范数。它是L1和L2正则项的一般化形式，可以根据不同的p值调节模型的稀疏性和平滑性。当p=0时，Lp正则项等价于L0正则项，即模型参数中非零参数的个数；当p=1时，等价于L1正则项；当p=2时，等价于L2正则项；当p趋近于无穷时，等价于最大范数（max norm），即模型参数中绝对值最大的那个值。

loss正则化   [模型训练——Loss函数](https://zhuanlan.zhihu.com/p/436809988)
正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。给loss function加上正则化项，**能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off）**，如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。[L1正则化与L2正则化](https://zhuanlan.zhihu.com/p/35356992)

pytorch实现 L2正则化 weight_decay用于设置权值衰减率，即正则化中的超参 ，默认值为0。
```python
e.g. optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.01)
```


（1） L1、L2区别  
L1是模型各个参数的绝对值之和。  
L2是模型各个参数的平方和的开方值。  
L1会趋向于产生少量的特征，而其他的特征都是0。  
因为最优的参数值很大概率出现在坐标轴上，这样就会导致某一维的权重为0 ，产生稀疏权重矩阵。  
L2会选择更多的特征，这些特征都会接近于0。  
最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化||w||时，就会使每一项趋近于0。  
（2） 优势

![](https://pic2.zhimg.com/80/v2-ef4f8e89e7be234d103b0cd1478ab5dd_720w.webp)

[Algorithm_Interview_Notes-Chinese/A-深度学习基础.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md#l1l2-%E8%8C%83%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96)

**为什么 L1 和 L2 正则化可以防止过拟合？**

-   L1 & L2 正则化会使模型偏好于更小的权值。
-   更小的权值意味着**更低的模型复杂度**；添加 L1 & L2 正则化相当于为模型添加了某种**先验**，限制了参数的分布，从而降低了模型的复杂度。
-   模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。——直观来说，就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声）——**奥卡姆剃刀原理**

**个人总结：使模型偏好与权值较小的目标函数 -> 更低的模型复杂度**  -> 提高泛化性 

**L1/L2 范数的作用、异同**

**相同点**

-   限制模型的学习能力——通过限制参数的规模，使模型偏好于**权值较小**的目标函数，防止过拟合。

**不同点**

-   **L1 正则化**可以产生更**稀疏**的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；**L2 正则化**主要用于防止模型过拟合
-   **L1 正则化**适用于特征之间有关联的情况；**L2 正则化**适用于特征之间没有关联的情况。

**为什么 L1 正则化可以产生稀疏权值，而 L2 不会？**

-   对目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数 `J` 的最小值
    
-   带有**L1 范数**（左）和**L2 范数**（右）约束的二维图示
    
    [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png) [![](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/raw/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/_assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png)
    
    -   图中 `J` 与 `L1` 首次相交的点即是最优解。`L1` 在和每个坐标轴相交的地方都会有“**顶点**”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 `J` 与这些“顶点”相交的机会远大于其他点，因此 `L1` 正则化会产生稀疏的解。
    -   `L2` 不会产生“**顶点**”，因此 `J` 与 `L2` 相交的点具有稀疏性的概率就会变得非常小。



[算法岗常见面试题（二）：正则化_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/468138806839926784?sourceSSR=users)

# 优化器

[一个框架看懂优化算法之异同 SGD/AdaGrad/Adam - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/32230623)


[算法岗常见面试题（六）：优化器_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/470889397320257536?sourceSSR=users)


优化器的作用：更新和计算影响模型训练和模型输出的网络参数，使其逼近或达到最优值，从而最小化（或最大化）损失函数。

发展路径：SGD -> SGDM -> NAG -> AdaGrad -> AdaDelta / RMSProp -> Adam -> Nadam -> AdamW

发展阶段：

-   没有动量：梯度下降<计算所有样本的梯度平均> / SGD<随机选择一个样本计算梯度> / 小批量随机梯度下降<每次处理一批样本>
-   一阶动量（惯性）：SGDM<梯度更新方向由历史梯度更新方向和当前梯度更新方向共同决定> / NAG<假设参数先按上一轮梯度更新方向变化，再计算当前梯度更新方向> **一阶动量是各个时刻梯度方向的指数移动平均值**
-   二阶动量（自适应学习率）：AdaGrad<迄今为止所有梯度的平方和> / AdaDelta / RMSProp<只关注过去一段时间内的梯度更新频率> 
- **二阶动量——该维度上，迄今为止所有梯度值的平方和 **
-   一阶动量+二阶动量：Adam<SGDM+AdaDelta> / Nadam<NAG+AdaDelta> / AdamW<Adam+L2正则化> 

![](https://uploadfiles.nowcoder.com/images/20230330/485173046_1680140690929/D2B5CA33BD970F64A6301FA75AE2EB22)

![](https://uploadfiles.nowcoder.com/images/20230330/485173046_1680140726796/D2B5CA33BD970F64A6301FA75AE2EB22)


Adam结合SGDM和AdaDelta两种优化算法的优点。对梯度的一阶动量（惯性）和二阶动量（更新频率）进行综合考虑，计算出更新步长。**一阶动量**的优势在于他能够学习到历史梯度下降的惯性，避免受到单个样本分布的干扰，**减少震荡**，加快收敛；**二阶动量**的优势在于是**自适应学习率**，为参数的不同维分配不同的学习率，在**模型稀疏的情况下效果很好**。

[Algorithm_Interview_Notes-Chinese/C-专题-优化算法.md at master · dqhplhzz2008/Algorithm_Interview_Notes-Chinese (github.com)](https://github.com/dqhplhzz2008/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/C-%E4%B8%93%E9%A2%98-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.md#%E4%B8%93%E9%A2%98-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)


# 几种卷积变体：空洞卷积、深度可分离卷积 因果卷积


因果卷积：
WAVENET中的扩张因果卷积，扩大感受野
如果考虑很久之前的变量x，就会导致卷积层数的增加。网络过于深会带来[梯度下降](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%25E6%25A2%25AF%25E5%25BA%25A6%25E4%25B8%258B%25E9%2599%258D%26spm%3D1001.2101.3001.7020)，训练复杂，拟合效果不好的问题，因此提出了空洞卷积。
[空洞卷积]([吃透空洞卷积(Dilated Convolutions) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/113285797))  增加感受野
在卷积核中增加空洞来增加感受野，不增加过多计算。普通卷积有着3 * 3的卷积核空洞卷积有着3 * 3的卷积核，空洞rate为2，可以使得神经网络在同样的层数下，拥有更大的感受野。  
空洞卷积存在的问题：  
空洞卷积的卷积核不连续，不是所有的信息参与了计算，导致信息连续性的损失，引起栅格效应。
[深度可分离卷积](https://zhuanlan.zhihu.com/p/166736637)   轻量级，**MobileNet**


# 转置卷积原理 如何用卷积实现

[(33条消息) 李沐动手学深度学习V2-转置卷积和代码实现_cv_lhp的博客-CSDN博客](https://blog.csdn.net/flyingluohaipeng/article/details/125230277#:~:text=%E5%AF%B9%E8%BE%93%E5%85%A5%E7%9F%A9%E9%98%B5X%E5%92%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9F%A9%E9%98%B5K%E5%AE%9E%E7%8E%B0%E5%9F%BA%E6%9C%AC%E7%9A%84%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%EF%BC%8C%E4%B8%8E%E9%80%9A%E8%BF%87%E5%8D%B7%E7%A7%AF%E6%A0%B8%E2%80%9C%E5%87%8F%E5%B0%91%E2%80%9D%E8%BE%93%E5%85%A5%E5%85%83%E7%B4%A0%E7%9A%84%E5%B8%B8%E8%A7%84%E5%8D%B7%E7%A7%AF%E7%9B%B8%E6%AF%94%EF%BC%8C%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E9%80%9A%E8%BF%87%E5%8D%B7%E7%A7%AF%E6%A0%B8%E2%80%9C%E5%B9%BF%E6%92%AD%E2%80%9D%E8%BE%93%E5%85%A5%E5%85%83%E7%B4%A0%EF%BC%8C%E4%BB%8E%E8%80%8C%E4%BA%A7%E7%94%9F%E5%A4%A7%E4%BA%8E%E8%BE%93%E5%85%A5%E7%9A%84%E8%BE%93%E5%87%BA%E3%80%82%20import%20torch%20import%20d2l.torch,from%20torch%20import%20nn%20%23%E6%AD%A4%E5%AE%9E%E7%8E%B0%E6%98%AF%E5%9F%BA%E6%9C%AC%E7%9A%84%E4%BA%8C%E7%BB%B4%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E3%80%82)

如果stride>1 需要插值 交叉补零
之后再在两边补零 然后将卷积核上下左右翻转 进行正常卷积
# 因果性


因果性，只能用过去的信息
![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230405/image.xospsy7s9lc.webp)


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230405/image.6r7tv8j6mo80.webp)


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230405/image.4klvixacxma0.webp)

# 感受野计算

感受野的计算公式如下：

![](https://www.nowcoder.com/equation?tex=l_%7Bk%7D%3Dl_%7Bk-1%7D%2B%5B(f_%7Bk%7D-1)*%5Cprod_%7Bi%3D1%7D%5E%7Bk-1%7Ds_i%5D)  
其中![](https://www.nowcoder.com/equation?tex=l_%7Bk%20%E2%88%92%201%7D&preview=true)为第![](https://www.nowcoder.com/equation?tex=k%E2%88%921&preview=true)层对应的感受野大小，![](https://www.nowcoder.com/equation?tex=f_k&preview=true)为第![](https://www.nowcoder.com/equation?tex=k&preview=true)层的卷积核大小，或者是池化层的池化尺寸大小，![](https://www.nowcoder.com/equation?tex=s_i&preview=true)为步长。

卷积网络中，三个5x5(stride 1)卷积核级联最大可以获得多大的有效感受野？

  
记公式太麻烦了，举个例子：**假如输入尺寸为24×24,3个5×5卷积核得到的输出尺寸为，24-5+1=20,20-5+1=16,16-5+1=12。就相当于24-13+1=12，也就是3个5×5卷积核可以替换13×13的卷积核**。

# 计算量

卷积计算量 n * m * k * k * cin * cout
**计算量越小，模型推理就越快吗** 不是的 **模型在特定硬件上的推理速度，除了受计算量影响外，还会受访存量、硬件特性、软件实现、系统环境等诸多因素影响**

[教你如何估计各种神经网络的计算量和参数量 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/342668070)
全连接 LSTM  卷积的计算量

FLOPS 怎么计算 计算出来是1s的音频

**全连接**

![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230724/image.5ue9f5xv2gw0.png)

**激活函数**

![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230724/image.3xlw40yco4e0.webp)

**LSTM**

![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230724/image.5s2rifpy9040.webp)

**卷积**


![image](https://cdn.jsdelivr.net/gh/andyye1999/picx-images-hosting@master/20230724/image.4ksv7b5hmky0.webp)


# CNN和RNN的区别和使用场景

CNN是卷积神经网络的简称，卷积神经网络一般由三个部分组成--输入层，输出层，和隐藏层。其核心的操作是卷积操作。卷积操作本质上是一种分组函数，CNN使用卷积来筛选数据并查找信息。卷积神经网络的特点是使用固定大小的输入和输出，并且学习的特征倾向于空间特征，因此卷积神经网络更适用于图像和视频的处理。RNN是循环神经网络的简称，与卷积神经网络不同的是，RNN中存在记忆单元，可以接受之前的信息指导当前的数据处理。循环神经网络的特点是可以接受变长的输出并产生变长的输出，并且学习的特征倾向于时间特征，或者说序列特征，因此循环神经网络更适用与语音和文本的处理。但是其实在具体领域这两者的应用没有具体分界，谁的性能好就用谁呗。


# 激活函数以及优缺点

[[深度学习中的激活函数]]



# self atention & transformer

[超详细图解Self-Attention - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/410776234)

[熬了一晚上，我从零实现了Transformer模型，把代码讲给你听 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/411311520)

[算法岗常见面试题（八）：Transformer_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/473903838680875008?sourceSSR=search)

# 交叉熵

[损失函数｜交叉熵损失函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/35709485)

你使用mse对分类任务时，梯度更新的时候是会涉及到sigmoid的计算，而你用交叉熵的话求梯度更新只涉及到真实值，预测值和x，计算就会实现简单方便
- 逻辑回归使用mse为损失函数时，是非凸的损失函数，不方便优化；而对数损失(二元交叉熵损失)能保证损失函数为凸函数，优化到全局最低点。
MSE + sigmoid、交叉熵 + sigmoid配套。后者计算少1个sigmoid的导数项，梯度等于预测值和真实值差值乘以x，计算更方便，而且差值越大代表梯度越大，更新越快，符合物理意义；前者sigmoid导数取值范围为[0,1]，计算复杂，而且可能出现梯度消失
交叉熵实际上是对输入数据的概率按照 p_i ^ y_i 的形式做了一个极大似然估计，取个log就是交叉熵的形式了。 而 mse 是假定误差服从高斯分布，简单来说是对 exp(|y - y'|**2) 进行极大似然估计，取个log得到平方损失的形式

# pooling方式 pooling方向传播过程

[(34条消息) Max Pooling和 Average Pooling的区别，使用场景分别是什么？_maxpooling_ytusdc的博客-CSDN博客](https://blog.csdn.net/ytusdc/article/details/104415261#:~:text=1%20%E9%80%9A%E5%B8%B8%E6%9D%A5%E8%AE%B2%EF%BC%8Cmax-pooling%E7%9A%84%E6%95%88%E6%9E%9C%E6%9B%B4%E5%A5%BD%EF%BC%8C%E8%99%BD%E7%84%B6max-pooling%E5%92%8Caverage-pooling%E9%83%BD%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%81%9A%E4%BA%86%E4%B8%8B%E9%87%87%E6%A0%B7%EF%BC%8C%E4%BD%86%E6%98%AFmax-pooling%E6%84%9F%E8%A7%89%E6%9B%B4%E5%83%8F%E6%98%AF%E5%81%9A%E4%BA%86%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%8C%E9%80%89%E5%87%BA%E4%BA%86%E5%88%86%E7%B1%BB%E8%BE%A8%E8%AF%86%E5%BA%A6%E6%9B%B4%E5%A5%BD%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E6%8F%90%E4%BE%9B%E4%BA%86%E9%9D%9E%E7%BA%BF%E6%80%A7%E3%80%82%20...%202,average-pooling%E6%9B%B4%E5%BC%BA%E8%B0%83%E5%AF%B9%E6%95%B4%E4%BD%93%E7%89%B9%E5%BE%81%E4%BF%A1%E6%81%AF%E8%BF%9B%E8%A1%8C%E4%B8%80%E5%B1%82%E4%B8%8B%E9%87%87%E6%A0%B7%EF%BC%8C%E5%9C%A8%E5%87%8F%E5%B0%91%E5%8F%82%E6%95%B0%E7%BB%B4%E5%BA%A6%E7%9A%84%E8%B4%A1%E7%8C%AE%E4%B8%8A%E6%9B%B4%E5%A4%A7%E4%B8%80%E7%82%B9%EF%BC%8C%E6%9B%B4%E5%A4%9A%E7%9A%84%E4%BD%93%E7%8E%B0%E5%9C%A8%E4%BF%A1%E6%81%AF%E7%9A%84%E5%AE%8C%E6%95%B4%E4%BC%A0%E9%80%92%E8%BF%99%E4%B8%AA%E7%BB%B4%E5%BA%A6%E4%B8%8A%EF%BC%8C%E5%9C%A8%E4%B8%80%E4%B8%AA%E5%BE%88%E5%A4%A7%E5%BE%88%E6%9C%89%E4%BB%A3%E8%A1%A8%E6%80%A7%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%B8%AD%EF%BC%8C%E6%AF%94%E5%A6%82%E8%AF%B4DenseNet%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E5%A4%A7%E5%A4%9A%E9%87%87%E7%94%A8average-pooling%EF%BC%8C%E5%9C%A8%E5%87%8F%E5%B0%91%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%90%8C%E6%97%B6%EF%BC%8C%E6%9B%B4%E6%9C%89%E5%88%A9%E4%BF%A1%E6%81%AF%E4%BC%A0%E9%80%92%E5%88%B0%E4%B8%8B%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E3%80%82%203%20average-pooling%E5%9C%A8%E5%85%A8%E5%B1%80%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%E6%93%8D%E4%BD%9C%E4%B8%AD%E5%BA%94%E7%94%A8%E4%B9%9F%E6%AF%94%E8%BE%83%E5%B9%BF%EF%BC%8C%E5%9C%A8ResNet%E5%92%8CInception%E7%BB%93%E6%9E%84%E4%B8%AD%E6%9C%80%E5%90%8E%E4%B8%80%E5%B1%82%E9%83%BD%E4%BD%BF%E7%94%A8%E4%BA%86%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%E3%80%82%20)

pooling作用 ：下采样 降维 去除冗余信息
实现非线性，防止过拟合
实现特征不变性  其中不变形性包括，平移不变性、旋转不变性和尺度不变性。池化操作操作使模型更关注是否存在某些特征而不是特征具体的位置。可看作是一种很强的先验，使特征学习包含某种程度自由度，能容忍一些特征微小的位移

根据相关理论，特征提取的误差主要来自两个方面：

- 邻域大小受限造成的估计值方差增大；
- 卷积层参数误差造成估计均值的偏移。

一般来说，average-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息
  

[(34条消息) 池化层（pooling）的反向传播是怎么实现的_前向池化和反向池化_妈妈说名字要起的长看起来才够叼的博客-CSDN博客](https://blog.csdn.net/Jason_yyz/article/details/80003271)
mean pooling的前向传播就是把一个patch中的值求取平均来做pooling，那么反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变
max pooling也要满足梯度之和不变的原则 ，max pooling的前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是 把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0 。所以max pooling操作和mean pooling操作不同点在于需要记录下池化操作时到底哪个像素的值是最大，也就是max id ，这个变量就是记录最大值所在位置的，因为在反向传播中要用到

# 互相关




# 阵列幅度失真



幅度失真是因为时延估计和噪声协方差矩阵不准确造成的。而之所以不准确是因为，**时延估计和噪声协方差矩阵不准确**会造成**波束的主瓣方向出现偏差**，使**主瓣方向无法对准声源方向**，这样的话语音就会被抑制。


# PCM和WAV文件中存储的是啥

[[PCM WAV]]
PCM的存储方式为小端模式 

先低字节 再高字节 
如果是双声道 先左底 左高 右底 右高
# 采样率和采样深度的物理意义


采样位深，音频的位深度决定动态范围。我们常见的16Bit（16比特），可以记录大概96分贝的动态范围。那么，您可以大概知道，**每一个比特大约可以记录6分贝的声音**。同理，20Bit可记录的动态范围大概就是120dB；24Bit就大概是144dB。音频位速，也叫码率，或者比特率。位速是指在一个数据流中每秒钟能通过的信息量，也可以理解为：每秒钟用多少比特的数据量去表示。96kbps的WMA音频格式的音质明显要比96kbps的MP3音质好。为什么会这样呢？因为不同的压缩算法，对数据的利用率不同而造成的差异。再举例，假如MP3压缩至48kbps以下，已经惨不忍睹，而如果是AAC音频格式，同样是48kbps的位速下，音质明显比MP3好。

**dbfs**
先科普一下样本点幅度值 **Sample** 与分贝 **dB** 之间的关系，以 16bit 量化的音频采样点为例：**dB = 20 * log10（Sample / 32768.0）**，与 Adobe Audition 右侧纵坐标刻度一致。  
幅度值表示：16bit 采样最小值为 0，最大值绝对值为 32768（**幅度值如下图右边栏纵坐标**）。


# 128点FFT变换，16kHz的音频，可以分为多少个子带？ 频率分辨率

8 / 64 

# 维纳滤波和谱减法的区别



# 时延估计的实现



# WebRTC NS中的噪声估计用到的三个特征值是什么？它们分别的定义是什么？

[[webrtc_ns#FeatureUpdate() 提取平均LRT参数、频谱差异、频谱平坦度]]

# 你用的VAD的是做什么用的，原理是什么，遇到突发噪声怎么处理？

[[语音活动检测模块]]

This is a Voice Activity Detection (VAD) code in C language. The code performs various operations to detect voice activity in an audio signal.

1.  FFT (Fast Fourier Transform) is performed on the input data buffer using the "r_fft" function with a flag value of +1.
    
2.  Energy in each channel is estimated by looping over the specified channels and computing the sum of squares of the real and imaginary parts of the FFT coefficients. The estimated energy is then smoothed using a constant value "CEE_SM_FAC".
    
3.  Channel noise is estimated for the first four frames by taking the maximum value between the channel energy and a constant "INE".通过取信道能量和常数“INE”之间的最大值来估计前四帧的信道噪声。
    
4.  The Signal-to-Noise Ratio (SNR) for each channel is calculated by dividing the channel energy by the channel noise and converting the result to dB.每个通道的信噪比 (SNR) 是通过将通道能量除以通道噪声并将结果转换为 dB 来计算的。
    
5.  The sum of voice metrics is calculated using a table "vm_tbl" that maps SNR values to voice metrics.使用将 SNR 值映射到语音指标的表“vm_tbl”计算语音指标的总和。
    
6.  The total noise estimate (tne) and total channel energy estimate (tce) are calculated by summing the channel noise and channel energy values respectively.总噪声估计 (tne) 和总信道能量估计 (tce) 分别通过对信道噪声和信道能量值求和来计算。
    
7.  Log spectral deviation is calculated by subtracting the long-term log spectral energy from the current log spectral energy and summing the absolute values.通过从当前对数光谱能量中减去长期对数光谱能量并将绝对值相加来计算对数光谱偏差。
    
8.  The long-term integration constant (alpha) is calculated based on the total channel energy estimate (tce). A higher total channel energy results in a slower integration and vice-versa.长期积分常数 (alpha) 是根据总信道能量估计 (tce) 计算的。 更高的总通道能量导致更慢的集成，反之亦然。
    
9.  Long-term log spectral energy is calculated by applying the integration constant to the current log spectral energy.通过将积分常数应用于当前对数光谱能量来计算长期对数光谱能量。
    
10.  The update flag is set or reset based on the sum of voice metrics and the total channel energy estimate.更新标志是根据语音度量和总信道能量估计的总和来设置或重置的。
    
11.  The modify flag is set or reset based on the current and previous update counts.
    

The purpose of the code is to determine whether an audio frame contains voice or non-voice content. The update and modify flags can be used to trigger additional processing such as speech enhancement, noise reduction, etc.


# 麦克风阵列的物理结构特性有哪些指标？你的麦克风阵列的指标都是怎样的？


麦克风阵列的物理结构特性的指标有以下几点：

1.  指向性：指麦克风阵列能够捕捉到的声音的方向性。一般情况下，麦克风阵列的指向性与麦克风的布局有关。
    
2.  分辨率：指麦克风阵列能够区分声音的位置的精度。麦克风阵列的分辨率取决于麦克风数量和布局。
    
3.  灵敏度：指麦克风阵列能够捕捉到声音的最小强度。灵敏度越高，麦克风阵列能够捕捉到的声音越弱。
    
4.  噪声级：指麦克风阵列自身产生的噪声。噪声级越低，麦克风阵列的性能越好。
    
5.  动态范围：指麦克风阵列能够捕捉到的声音的最大和最小强度差。动态范围越大，麦克风阵列的性能越好。
    
6.  过滤效果：指麦克风阵列能够滤掉外界杂音的效果。过滤效果越好，麦克风阵列的性能越好。
    
7.  抗干扰能力：指麦克风阵列在干扰环境下的工作效果。抗干扰能力越强，麦克风阵列的性能越好。

以上是由chatgpt回答


# 都用了哪些噪声种类？现在机器学习几百种噪声，为什么用这么少？



# 在哪些噪声环境下语音增强的效果比较差？



# 增强语音的结果的衡量标准是什么？


语音增强结果的衡量标准主要包括：

1.  听觉评价：根据人耳的听觉感受来评价语音的质量。例如 Mean Opinion Score (MOS)。
    
2.  信噪比：语音信号与噪声的比值。
    
3.  声音干净度：语音信号中的噪声减少了多少。
    
4.  语音信息内容保留度：增强语音信号与原始语音信号的相似度。
    
5.  语音特征保留度：语音增强后语音特征的保留情况，例如语音语调、语速、音高、音色等。

不同的语音增强任务可能需要不同的衡量标准，对于每一种语音增强算法，其衡量标准通常要结合其具体任务和应用场景进行评估。
以上由chatgpt进行回答

ASR  

PESQ STOI MOS  SISNR   DNSMOS参赛队伍才有资格
带宽扩展中有log-spectral distance (LSD) high-frequency log-spectral distance (LSD-HF) 

**如何判断噪声估计是否准，可以加入均值方差已知的高斯白噪声来看估计的噪声准不准**

#  k-means原理及实现（会不会出现一类没有信息

**（1）原理**

K-means算法是最常用的一种[聚类算法](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%25E8%2581%259A%25E7%25B1%25BB%25E7%25AE%2597%25E6%25B3%2595%26spm%3D1001.2101.3001.7020)。算法的输入为一个样本集（或者称为点集），通过该算法可以将样本进行聚类，具有相似特征的样本聚为一类。针对每个点，计算这个点距离所有中心点最近的那个中心点，然后将这个点归为这个中心点代表的簇。一次迭代结束之后，针对每个簇类，重新计算中心点，然后针对每个点，重新寻找距离自己最近的中心点。如此循环，直到前后两次迭代的簇类没有变化。  
下面通过一个简单的例子，说明K-means算法的过程。如下图所示，目标是将样本点聚类成3个类别。

![](https://pic4.zhimg.com/80/v2-4cb4c9faeb6021f862ac3f9add371547_720w.webp)

**（2）算法流程**

选择聚类的个数k（kmeans算法传递超参数的时候，只需设置最大的K值）  
任意产生k个聚类，然后确定聚类中心，或者直接生成k个中心。  
对每个点确定其聚类中心点。  
再计算其聚类新中心。  
重复以上步骤直到满足收敛要求。（通常就是确定的中心点不再改变。）  
**上述步骤的关键两点是：**找到距离自己最近的中心点。更新中心点。

**（3）python实现**

```python
# K-means Algorithm is a clustering algorithm
import numpy as np
import matplotlib.pyplot as plt
import random
 
 
def get_distance(p1, p2):
    diff = [x-y for x, y in zip(p1, p2)]
    distance = np.sqrt(sum(map(lambda x: x**2, diff)))
    return distance
 
 
# 计算多个点的中心
# cluster = [[1,2,3], [-2,1,2], [9, 0 ,4], [2,10,4]]
def calc_center_point(cluster):
    N = len(cluster)
    m = np.matrix(cluster).transpose().tolist()
    center_point = [sum(x)/N for x in m]
    return center_point
 
 
# 检查两个点是否有差别
def check_center_diff(center, new_center):
    n = len(center)
    for c, nc in zip(center, new_center):
        if c != nc:
            return False
    return True
 
 
# K-means算法的实现
def K_means(points, center_points):
 
    N = len(points)         # 样本个数
    n = len(points[0])      # 单个样本的维度
    k = len(center_points)  # k值大小
 
    tot = 0
    while True:             # 迭代
        temp_center_points = [] # 记录中心点
 
        clusters = []       # 记录聚类的结果
        for c in range(0, k):
            clusters.append([]) # 初始化
 
        # 针对每个点，寻找距离其最近的中心点（寻找组织）
        for i, data in enumerate(points):
            distances = []
            for center_point in center_points:
                distances.append(get_distance(data, center_point))
            index = distances.index(min(distances)) # 找到最小的距离的那个中心点的索引，
 
            clusters[index].append(data)    # 那么这个中心点代表的簇，里面增加一个样本
 
        tot += 1
        print(tot, '次迭代   ', clusters)
        k = len(clusters)
        colors = ['r.', 'g.', 'b.', 'k.', 'y.']  # 颜色和点的样式
        for i, cluster in enumerate(clusters):
            data = np.array(cluster)
            data_x = [x[0] for x in data]
            data_y = [x[1] for x in data]
            plt.subplot(2, 3, tot)
            plt.plot(data_x, data_y, colors[i])
            plt.axis([0, 1000, 0, 1000])
 
        # 重新计算中心点（该步骤可以与下面判断中心点是否发生变化这个步骤，调换顺序）
        for cluster in clusters:
            temp_center_points.append(calc_center_point(cluster))
 
        # 在计算中心点的时候，需要将原来的中心点算进去
        for j in range(0, k):
            if len(clusters[j]) == 0:
                temp_center_points[j] = center_points[j]
 
        # 判断中心点是否发生变化：即，判断聚类前后样本的类别是否发生变化
        for c, nc in zip(center_points, temp_center_points):
            if not check_center_diff(c, nc):
                center_points = temp_center_points[:]   # 复制一份
                break
        else:   # 如果没有变化，那么退出迭代，聚类结束
            break
 
    plt.show()
    return clusters # 返回聚类的结果
 
# 随机获取一个样本集，用于测试K-means算法
def get_test_data():
 
    N = 1000
 
    # 产生点的区域
    area_1 = [0, N / 4, N / 4, N / 2]
    area_2 = [N / 2, 3 * N / 4, 0, N / 4]
    area_3 = [N / 4, N / 2, N / 2, 3 * N / 4]
    area_4 = [3 * N / 4, N, 3 * N / 4, N]
    area_5 = [3 * N / 4, N, N / 4, N / 2]
 
    areas = [area_1, area_2, area_3, area_4, area_5]
    k = len(areas)
 
    # 在各个区域内，随机产生一些点
    points = []
    for area in areas:
        rnd_num_of_points = random.randint(50, 200)
        for r in range(0, rnd_num_of_points):
            rnd_add = random.randint(0, 100)
            rnd_x = random.randint(area[0] + rnd_add, area[1] - rnd_add)
            rnd_y = random.randint(area[2], area[3] - rnd_add)
            points.append([rnd_x, rnd_y])
 
    # 自定义中心点，目标聚类个数为5，因此选定5个中心点
    center_points = [[0, 250], [500, 500], [500, 250], [500, 250], [500, 750]]
 
    return points, center_points
 
 
if __name__ == '__main__':
 
    points, center_points = get_test_data()
    clusters = K_means(points, center_points)
    print('#######最终结果##########')
    for i, cluster in enumerate(clusters):
        print('cluster ', i, ' ', cluster)
```

**（4）会不会出现一类空？**

会，如果两类之间距离太近





