# 神经网络权重的定点化


先将数据先进行定点化再还原，例如先乘8192取short再除8192，这样网络就会将损失的精度进行学习

_ norm函数  冗余符号位

自己写的网络定点化能到达50信噪比

![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20221109/image.3faruranilq0.webp)
![image](https://cdn.jsdelivr.net/gh/andyye1999/image-hosting@master/20221109/image.68yabu86p1s0.webp)


# 定点化技巧

统计降噪的定点化FFT中+16384的目的是  在进行右移的同时，因为运算结果是向下取整的，为了避免因为右移造成的精度损失，所以在每个数的末尾加上一个偏移量，即16384L，以便获得一个更精确的结果。相当于+0.5 为了四舍五入 得到更高精度 尤其是像自适应滤波器或者FFT这种循环迭代的定点化  这么做的目的是为了使运算结果四舍五入为最接近的整数，以保证运算结果的精度。

webrtc的NSX中fft定点化inst->stages = inst->order 80点->128FFt 为7 160点->256FFT 为8
FFT的定点化之后的Q值为(norm-stages)

窗函数或者查表的数组浮点为1时，定点为32767，不是32768

log函数怎么实现看那个定点化文档的PDF，最后有详细解释
log10(x) = log10(2) *  log2(x)

用电脑去模拟指令集，芯片是恒玄2500，arm指令集。32位分为高16位低16位
basic_op.c 这个文件中有以下几种函数：

add、sub、abs_s、shl、shr等，用于对16位整数进行加减、绝对值、左移、右移等运算。123
L_add、L_sub、L_abs、L_shl、L_shr等，用于对32位整数进行加减、绝对值、左移、右移等运算。123
mult、round等，用于对16位整数进行乘法和四舍五入运算，并返回16位整数。123
L_mult等，用于对16位整数进行乘法运算，并返回32位整数。123
mac_r、msu_r等，用于对两个16位整数进行乘累加或乘累减运算，并返回16位整数。123
L_mac、L_msu等，用于对两个16位整数进行乘累加或乘累减运算，并返回32位整数

mpy32_16 相当于32位乘16位数再右移15位
mpy32_32 相当于32位乘32位数再右移31位
div_s 除法 将分母取norm 上面的1再右移norm位 得到结果是Q15的数 函数的分子必须小于分母

DIV_32类似，32位的除法
除法转换为乘法，但Qzhi还是按除法算，即除法后的Q 是 之前的Q减去分母norm之前的Q值

长时平滑agc定点化函数
平方根[[平方根]]

# 定点化误差 无损定点化

[浮点数的定点化 - Xiezq97 - 博客园 (cnblogs.com)](https://www.cnblogs.com/Xiezq97/p/16322136.html)

无损定点化就是能去的最大Q值 
1、【大疆】对 12.918 做无损定点化，需要的最小位宽是多少位？位宽选择 11 位时的量化误差是多少？

A   13位，0.0118
B   12位，0.0118
C   13位，0.0039
D   12位，0.0039

　　分析：（1）整数 12 需要 4bit。假设位宽选择12位，即小数需要 8bit，12.918 * 28 = 3307.008，定点化后的小数小于0.5，可以看成是【无损定点化】，固答案为 12 位；（2）位宽选择11位，即小数需要 7bit，12.918 * 27 = 1653.504，量化误差为 0.504/27 = 0.0039375，固答案为0.0039。